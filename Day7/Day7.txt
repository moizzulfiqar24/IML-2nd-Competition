7.2

Removed Object Dtypes
StandardScaler
Keras With Torch Backend
1 Hidden Layer: Hidden Layer 1 with 140 neurons relu activation, Output linear
Adam Optimizer
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping, dropout=0.3 (used in hidden layers only)

7.3

Removed Object Dtypes
StandardScaler
Keras With Torch Backend
2 Hidden Layers: Hidden Layer 1 with 100 neurons relu activation, 
Hidden Layer 2 with 70 neurons relu activation, Dropout 0.3, Output linear
Optimizer: Adam
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping

7.4

Removed Object Dtypes
StandardScaler
Keras With Torch Backend
2 Hidden Layers: Hidden Layer 1 with 100 neurons relu activation, Hidden Layer 2 with 50 neurons relu activation, Dropout 0.3, Output linear
Optimizer: Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping

7.5

Removed sub_area
OneHot Encoding
Converted All Columns To float32
StandardScaler
Keras With Torch Backend
2 Hidden Layers: Hidden Layer 1 with 100 neurons relu activation, Hidden Layer 2 with 50 neurons relu activation, Dropout 0.3, Output linear
Optimizer: Adam(lr=0.01, epsilon=1e-07)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping, dropout=0.3 (used in hidden layers only)