11.1

Features Used = 271

Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.05)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

11.2 (Best)

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.05)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

11.3

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
MLPRegressor
4 Hidden Layers: Hidden Layer 1 with 300 neurons tanh activation L2 regularization, 
Hidden Layer 2 with 210 neurons tanh activation L2 regularization, 
Hidden Layer 3 with 150 neurons tanh activation L2 regularization, 
Hidden Layer 4 with 60 neurons tanh activation L2 regularization
Optimizer: adam
batch_size=1800, epochs=100, early_stopping

10.4

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
MLPRegressor
4 Hidden Layers: Hidden Layer 1 with 300 neurons logistic activation L2 regularization, 
Hidden Layer 2 with 210 neurons logistic activation L2 regularization, 
Hidden Layer 3 with 150 neurons logistic activation L2 regularization, 
Hidden Layer 4 with 60 neurons logistic activation L2 regularization
Optimizer: adam
batch_size=1800, epochs=100, early_stopping

10.5

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
MLPRegressor
4 Hidden Layers: Hidden Layer 1 with 300 neurons identity activation L2 regularization, 
Hidden Layer 2 with 210 neurons identity activation L2 regularization, 
Hidden Layer 3 with 150 neurons identity activation L2 regularization, 
Hidden Layer 4 with 60 neurons identity activation L2 regularization
Optimizer: adam
batch_size=1800, epochs=100, early_stopping

11.6

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
MLPRegressor
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, 
Hidden Layer 2 with 210 neurons relu activation L2 regularization, 
Hidden Layer 3 with 150 neurons relu activation L2 regularization, 
Hidden Layer 4 with 60 neurons relu activation L2 regularization
Optimizer: adam
batch_size=1800, epochs=100, early_stopping

11.7 (Best)

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.1)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

11.8 (Best)

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.2)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

11.9 (Best)

Features Used = 270

Removed sub_area
Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.4)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization