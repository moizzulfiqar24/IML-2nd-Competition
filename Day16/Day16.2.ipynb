{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop, Adagrad, Nadam, Ftrl\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test_data = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "\n",
    "used = []\n",
    "\n",
    "# Extract features and target variable\n",
    "X = train_data.drop('price_doc', axis=1)\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('sub_area', axis=1)\n",
    "X_test = X_test.drop('sub_area', axis=1)\n",
    "used.append('Removed sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(X_test) \n",
    "used.append('OneHot Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import LabelEncoder\n",
    "\n",
    "# categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# categorical_columns_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     X_test[column] = label_encoder.fit_transform(X_test[column])\n",
    "\n",
    "# used.append('Label Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all columns in X_train with dtypes object\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         X.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# # drop all columns in X_test with dtypes object\n",
    "# for col in X_test.columns:\n",
    "#     if X_test[col].dtype == 'object':\n",
    "#         X_test.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# used.append(\"Removed Object Dtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "# X_test = X_test[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "\n",
    "# used.append(\"Forward Feature Selection (n=14)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "threshold_value = 4000000\n",
    "variance_filter = VarianceThreshold(threshold=threshold_value)\n",
    "\n",
    "X = variance_filter.fit_transform(X)\n",
    "X_test = variance_filter.transform(X_test)\n",
    "\n",
    "used.append(\"Variance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# # Add a constant term to the feature matrix\n",
    "# X_with_const = sm.add_constant(X)\n",
    "\n",
    "# # Fit a linear regression model\n",
    "# model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# # Get p-values for each feature\n",
    "# p_values = model.pvalues[1:]  # Exclude the constant term\n",
    "\n",
    "# # Set your desired threshold for p-value\n",
    "# threshold = 0.00001\n",
    "\n",
    "# # Filter features based on p-value\n",
    "# selected_features = p_values[p_values < threshold].index\n",
    "\n",
    "# # Display selected features\n",
    "# print(\"Selected Features:\")\n",
    "# print(selected_features)\n",
    "# print(len(selected_features))\n",
    "\n",
    "# # Select columns in the DataFrame\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"P-Value Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the Random Forest model\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, bootstrap=True)\n",
    "\n",
    "# # Train the model to get feature importances\n",
    "# rf_model.fit(X, y)\n",
    "\n",
    "# # Feature importance analysis\n",
    "# feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# # Create a DataFrame with feature names and their importance scores\n",
    "# feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# # Sort the features based on importance in descending order\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Select the top N most important features\n",
    "# top_n_features = 20  # You can choose the number of top features based on your preference\n",
    "# selected_features = feature_importance_df.head(top_n_features)['Feature'].tolist()\n",
    "\n",
    "# # Subset the original data to include only the selected features\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"Random Forest Feature Importance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=10)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=10)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.log1p(X)\n",
    "X_test = np.log1p(X_test)\n",
    "used.append('log Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "used.append(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127054, 41)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)#, interaction_only=True)\n",
    "# X_train = poly.fit_transform(X_train)\n",
    "# X_test = poly.fit_transform(X_test)\n",
    "# used.append('PolynomialFeatures W/O Interaction')\n",
    "# # used.append('PolynomialFeatures With Interaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save X_train to csv\n",
    "# X_train.to_csv('train with poly w/o int.2.csv', index=False)\n",
    "# X_test.to_csv('test with poly w/o int.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=200)\n",
    "# principalComponents = pca.fit_transform(X_train)\n",
    "# X_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=200)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=200)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 415963037771643.7500            5.40m\n",
      "         2 366684327730950.3750            5.57m\n",
      "         3 326679544688622.8750            6.53m\n",
      "         4 293731456850021.5625            6.16m\n",
      "         5 266807346797187.0000            6.10m\n",
      "         6 244950794355462.2812            5.82m\n",
      "         7 226152684709528.8750            5.98m\n",
      "         8 211274848045175.8125            6.00m\n",
      "         9 199141376970842.8125            5.95m\n",
      "        10 188879529949815.9375            6.02m\n",
      "        11 180694497076845.7188            5.96m\n",
      "        12 173109661178865.1562            5.82m\n",
      "        13 167545495283535.5312            5.62m\n",
      "        14 162800942563895.0938            5.52m\n",
      "        15 158820278393002.9688            5.42m\n",
      "        16 154618938583246.3438            5.32m\n",
      "        17 151464479963901.8750            5.24m\n",
      "        18 147704235103038.9375            5.11m\n",
      "        19 144331343825680.4688            4.99m\n",
      "        20 140546360511034.7969            4.94m\n",
      "        21 137695251511955.0625            4.90m\n",
      "        22 135089312653469.8594            5.11m\n",
      "        23 132735928500835.9531            5.17m\n",
      "        24 130367170261067.2500            5.14m\n",
      "        25 128055130144566.6875            5.10m\n",
      "        26 125904552170696.5000            5.08m\n",
      "        27 123679829824796.7031            5.01m\n",
      "        28 122183490033104.4844            4.93m\n",
      "        29 120754361583777.0156            4.85m\n",
      "        30 119336955671539.5625            4.84m\n",
      "        31 117290166033822.6250            4.86m\n",
      "        32 115664784782174.0312            4.89m\n",
      "        33 114265666604754.6094            4.85m\n",
      "        34 113382350454278.1094            4.79m\n",
      "        35 111916908404686.0156            4.77m\n",
      "        36 110146849214928.6562            4.69m\n",
      "        37 108251451385738.6406            4.65m\n",
      "        38 106118186165533.0312            4.60m\n",
      "        39 104140362773514.7188            4.52m\n",
      "        40 103041978400338.4062            4.48m\n",
      "        41 101705198178633.8438            4.42m\n",
      "        42 100700395864667.2812            4.35m\n",
      "        43 98288204801046.1562            4.34m\n",
      "        44 96287770164027.8438            4.25m\n",
      "        45 95467916070213.4688            4.15m\n",
      "        46 93756986293098.6406            4.08m\n",
      "        47 92824249139424.3906            3.98m\n",
      "        48 91431237711251.7500            3.90m\n",
      "        49 90097023247584.3750            3.82m\n",
      "        50 88672887988091.7500            3.77m\n",
      "        51 88133776365759.4375            3.70m\n",
      "        52 87604217730271.9219            3.63m\n",
      "        53 86934744168063.0000            3.59m\n",
      "        54 86678408386305.1094            3.53m\n",
      "        55 86157409265528.6875            3.47m\n",
      "        56 85514320472908.8594            3.40m\n",
      "        57 85412702912830.0000            3.32m\n",
      "        58 84933613821685.7500            3.23m\n",
      "        59 84086524323078.1094            3.14m\n",
      "        60 84006453619418.5469            3.06m\n",
      "        61 83795675367134.6719            2.97m\n",
      "        62 83033887462683.3906            2.88m\n",
      "        63 82119015208411.7344            2.79m\n",
      "        64 81426711092916.1562            2.71m\n",
      "        65 81348724878226.5781            2.62m\n",
      "        66 80984221017612.4375            2.54m\n",
      "        67 80436784820435.5469            2.47m\n",
      "        68 79878373401047.2656            2.39m\n",
      "        69 79193811614765.8438            2.32m\n",
      "        70 78958787304546.2344            2.24m\n",
      "        71 78678184505931.1250            2.17m\n",
      "        72 78332974798196.1562            2.09m\n",
      "        73 77852117497871.2812            2.01m\n",
      "        74 77512099374258.4219            1.93m\n",
      "        75 77292573051611.6875            1.85m\n",
      "        76 76355928582881.7344            1.78m\n",
      "        77 75384217325673.4844            1.71m\n",
      "        78 75181454218128.4219            1.63m\n",
      "        79 74384085637200.0000            1.56m\n",
      "        80 73397415782558.9688            1.49m\n",
      "        81 73237436506763.7500            1.41m\n",
      "        82 72883042972698.8281            1.34m\n",
      "        83 71645238732865.8750            1.26m\n",
      "        84 71271012326293.9688            1.18m\n",
      "        85 70549121810133.1250            1.11m\n",
      "        86 70248980692003.7734            1.03m\n",
      "        87 69234670639943.9453           57.37s\n",
      "        88 68097733965903.2734           52.78s\n",
      "        89 67432528931338.1484           48.29s\n",
      "        90 66747505759734.7109           43.80s\n",
      "        91 65902533877433.0703           39.52s\n",
      "        92 65069097426989.7344           35.30s\n",
      "        93 64571623146264.0781           30.83s\n",
      "        94 64204708093873.6484           26.42s\n",
      "        95 63245551859232.7891           22.02s\n",
      "        96 62326466731231.6328           17.60s\n",
      "        97 62235109707303.2969           13.21s\n",
      "        98 61158050562838.2891            8.80s\n",
      "        99 60481478740054.1250            4.39s\n",
      "       100 60367203845762.9766            0.00s\n",
      "Validation Mean Squared Error: 177917481913624.22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rf_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error', verbose=2)\n",
    "used.append('GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss=\\'squared_error\\')')\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "print(f'Validation Mean Squared Error: {val_mse}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({'row ID': test_data['row ID'], 'price_doc': predictions.flatten()})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "submission_df.to_csv('Day16.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used = 41\n",
      "\n",
      "Removed sub_area\n",
      "OneHot Encoding\n",
      "Variance Based Feature Selection\n",
      "log Normalization\n",
      "StandardScaler\n",
      "GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error')\n"
     ]
    }
   ],
   "source": [
    "print(\"Features Used = \" + str(features_used) + \"\\n\")\n",
    "for i in used:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
