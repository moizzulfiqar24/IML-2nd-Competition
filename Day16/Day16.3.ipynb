{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop, Adagrad, Nadam, Ftrl\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test_data = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "\n",
    "used = []\n",
    "\n",
    "# Extract features and target variable\n",
    "X = train_data.drop('price_doc', axis=1)\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('sub_area', axis=1)\n",
    "X_test = X_test.drop('sub_area', axis=1)\n",
    "used.append('Removed sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(X_test) \n",
    "used.append('OneHot Encoding')\n",
    "\n",
    "X = X.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import LabelEncoder\n",
    "\n",
    "# categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# categorical_columns_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     X_test[column] = label_encoder.fit_transform(X_test[column])\n",
    "\n",
    "# used.append('Label Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all columns in X_train with dtypes object\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         X.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# # drop all columns in X_test with dtypes object\n",
    "# for col in X_test.columns:\n",
    "#     if X_test[col].dtype == 'object':\n",
    "#         X_test.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# used.append(\"Removed Object Dtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "# X_test = X_test[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "\n",
    "# used.append(\"Forward Feature Selection (n=14)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "# threshold_value = 4000000\n",
    "# variance_filter = VarianceThreshold(threshold=threshold_value)\n",
    "\n",
    "# X = variance_filter.fit_transform(X)\n",
    "# X_test = variance_filter.transform(X_test)\n",
    "\n",
    "# used.append(\"Variance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['full_sq', 'life_sq', 'floor', 'children_preschool',\n",
      "       'preschool_education_centers_raion',\n",
      "       'school_education_centers_top_20_raion', 'healthcare_centers_raion',\n",
      "       'university_top_20_raion', 'male_f', '0_6_female',\n",
      "       'build_count_monolith', 'raion_build_count_with_builddate_info',\n",
      "       'build_count_1971-1995', 'build_count_after_1995', 'kindergarten_km',\n",
      "       'green_zone_km', 'industrial_km', 'water_treatment_km', 'water_km',\n",
      "       'mkad_km', 'sadovoe_km', 'big_road2_km', 'nuclear_reactor_km',\n",
      "       'swim_pool_km', 'basketball_km', 'church_synagogue_km', 'catering_km',\n",
      "       'green_part_500', 'prom_part_500', 'trc_sqm_500',\n",
      "       'cafe_count_500_price_1000', 'cafe_count_500_price_1500',\n",
      "       'cafe_count_500_price_4000', 'cafe_count_500_price_high',\n",
      "       'mosque_count_500', 'leisure_count_500', 'market_count_500',\n",
      "       'green_part_1000', 'prom_part_1000', 'office_count_1000',\n",
      "       'office_sqm_1000', 'trc_count_1000', 'trc_sqm_1000',\n",
      "       'cafe_count_1000_price_1500', 'cafe_count_1000_price_high',\n",
      "       'big_church_count_1000', 'mosque_count_1000', 'leisure_count_1000',\n",
      "       'market_count_1000', 'green_part_1500', 'trc_count_1500',\n",
      "       'church_count_1500', 'mosque_count_1500', 'leisure_count_1500',\n",
      "       'green_part_2000', 'trc_sqm_2000', 'cafe_sum_2000_max_price_avg',\n",
      "       'cafe_count_2000_price_1500', 'cafe_count_2000_price_4000',\n",
      "       'cafe_count_2000_price_high', 'mosque_count_2000', 'sport_count_2000',\n",
      "       'mosque_count_3000', 'market_count_5000', 'product_type_Investment',\n",
      "       'product_type_OwnerOccupier', 'culture_objects_top_25_no',\n",
      "       'culture_objects_top_25_yes', 'incineration_raion_no',\n",
      "       'incineration_raion_yes', 'oil_chemistry_raion_no',\n",
      "       'oil_chemistry_raion_yes', 'radiation_raion_no',\n",
      "       'railroad_terminal_raion_yes', 'big_market_raion_no',\n",
      "       'big_market_raion_yes', 'nuclear_reactor_raion_no',\n",
      "       'nuclear_reactor_raion_yes', 'water_1line_no', 'water_1line_yes',\n",
      "       'big_road1_1line_no', 'big_road1_1line_yes', 'railroad_1line_yes',\n",
      "       'ecology_excellent', 'ecology_good', 'ecology_no data',\n",
      "       'ecology_satisfactory'],\n",
      "      dtype='object')\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant term to the feature matrix\n",
    "X_with_const = sm.add_constant(X)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# Get p-values for each feature\n",
    "p_values = model.pvalues[1:]  # Exclude the constant term\n",
    "\n",
    "# Set your desired threshold for p-value\n",
    "threshold = 0.00001\n",
    "\n",
    "# Filter features based on p-value\n",
    "selected_features = p_values[p_values < threshold].index\n",
    "\n",
    "# Display selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "print(len(selected_features))\n",
    "\n",
    "# Select columns in the DataFrame\n",
    "X = X[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "used.append(\"P-Value Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the Random Forest model\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, bootstrap=True)\n",
    "\n",
    "# # Train the model to get feature importances\n",
    "# rf_model.fit(X, y)\n",
    "\n",
    "# # Feature importance analysis\n",
    "# feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# # Create a DataFrame with feature names and their importance scores\n",
    "# feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# # Sort the features based on importance in descending order\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Select the top N most important features\n",
    "# top_n_features = 20  # You can choose the number of top features based on your preference\n",
    "# selected_features = feature_importance_df.head(top_n_features)['Feature'].tolist()\n",
    "\n",
    "# # Subset the original data to include only the selected features\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"Random Forest Feature Importance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=10)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=10)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.log1p(X)\n",
    "X_test = np.log1p(X_test)\n",
    "used.append('log Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "used.append(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127054, 87)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)#, interaction_only=True)\n",
    "# X_train = poly.fit_transform(X_train)\n",
    "# X_test = poly.fit_transform(X_test)\n",
    "# used.append('PolynomialFeatures W/O Interaction')\n",
    "# # used.append('PolynomialFeatures With Interaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save X_train to csv\n",
    "# X_train.to_csv('train with poly w/o int.2.csv', index=False)\n",
    "# X_test.to_csv('test with poly w/o int.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=200)\n",
    "# principalComponents = pca.fit_transform(X_train)\n",
    "# X_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=200)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=200)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1 415327695155180.5000           13.46m\n",
      "         2 365589450318067.6250           13.35m\n",
      "         3 325155743158195.8750           14.58m\n",
      "         4 291833414468864.3750           14.78m\n",
      "         5 264416595920218.0625           14.42m\n",
      "         6 242325955230458.5938           13.86m\n",
      "         7 224204954038374.9688           13.42m\n",
      "         8 209590478514171.8125           13.03m\n",
      "         9 197749011177485.4688           12.73m\n",
      "        10 187489604883376.9688           12.32m\n",
      "        11 179394539767333.3438           11.98m\n",
      "        12 172314993501754.0625           11.64m\n",
      "        13 166295146838121.8438           11.39m\n",
      "        14 161977571597355.5625           11.19m\n",
      "        15 157264370319932.0938           10.97m\n",
      "        16 153819518955249.0312           10.96m\n",
      "        17 150186821796117.1250           10.71m\n",
      "        18 146429496733663.5938           10.59m\n",
      "        19 143420988387798.8438           10.43m\n",
      "        20 140933124458999.3438           10.29m\n",
      "        21 137847941855591.8750           10.16m\n",
      "        22 134962819308654.8281           10.09m\n",
      "        23 133821794826937.3906            9.92m\n",
      "        24 131019796063230.6875            9.79m\n",
      "        25 129325073445308.2656            9.62m\n",
      "        26 126700766464802.7500            9.51m\n",
      "        27 125309054549795.2500            9.34m\n",
      "        28 123206311611867.8594            9.21m\n",
      "        29 121614653265547.6094            9.07m\n",
      "        30 120486031093330.8125            8.93m\n",
      "        31 119700712267990.8281            8.78m\n",
      "        32 117675479144291.6875            8.65m\n",
      "        33 115302412042335.9219            8.51m\n",
      "        34 114263770869932.3594            8.37m\n",
      "        35 112137024351920.3125            8.24m\n",
      "        36 110342308459660.4688            8.12m\n",
      "        37 109076790299650.0469            7.98m\n",
      "        38 108261293674811.8438            7.85m\n",
      "        39 107311537741841.4219            7.72m\n",
      "        40 105880956344086.0156            7.59m\n",
      "        41 104195065051877.1250            7.46m\n",
      "        42 102610720117732.5625            7.33m\n",
      "        43 101666081358726.7500            7.20m\n",
      "        44 98616761744302.0312            7.07m\n",
      "        45 96321975557306.2031            6.93m\n",
      "        46 94364363187806.7656            6.81m\n",
      "        47 92830659364361.0625            6.67m\n",
      "        48 90412828887425.5312            6.54m\n",
      "        49 89709468690955.3906            6.42m\n",
      "        50 88530061307545.1562            6.32m\n",
      "        51 87960011774089.0781            6.20m\n",
      "        52 86596107552830.9219            6.09m\n",
      "        53 86252282696174.6406            5.96m\n",
      "        54 85331154065190.3125            5.83m\n",
      "        55 83866498944744.8125            5.70m\n",
      "        56 83311918325026.5156            5.57m\n",
      "        57 82392047552629.7969            5.43m\n",
      "        58 82012278363474.6094            5.32m\n",
      "        59 80965727305130.4531            5.19m\n",
      "        60 80424899272018.9062            5.07m\n",
      "        61 80278411271619.9062            4.96m\n",
      "        62 80124474945578.1875            4.85m\n",
      "        63 79606207500889.9062            4.72m\n",
      "        64 78558996470318.6250            4.59m\n",
      "        65 77981620771647.9219            4.45m\n",
      "        66 77447515961683.4375            4.33m\n",
      "        67 76627413323666.7969            4.20m\n",
      "        68 76143881674520.0625            4.07m\n",
      "        69 75129432767236.0312            3.94m\n",
      "        70 74541053534076.3281            3.81m\n",
      "        71 74105263735520.6406            3.68m\n",
      "        72 73245685799032.9688            3.55m\n",
      "        73 71261862765904.3750            3.42m\n",
      "        74 70456545669687.7188            3.28m\n",
      "        75 69783472056332.3594            3.15m\n",
      "        76 68951301733100.9609            3.02m\n",
      "        77 67486190469977.6797            2.90m\n",
      "        78 66827316191198.1406            2.77m\n",
      "        79 66676278233445.0312            2.64m\n",
      "        80 66435692055591.0156            2.51m\n",
      "        81 66119016795973.0547            2.39m\n",
      "        82 65420048806101.9688            2.26m\n",
      "        83 64799641177989.3516            2.13m\n",
      "        84 64443432890921.0234            2.00m\n",
      "        85 64010023037691.4531            1.88m\n",
      "        86 63608083193163.5938            1.75m\n",
      "        87 63400749709981.1016            1.62m\n",
      "        88 62996601775716.5547            1.50m\n",
      "        89 62769798469732.6250            1.38m\n",
      "        90 62192525016839.5703            1.25m\n",
      "        91 61600823926413.5781            1.13m\n",
      "        92 61063407622010.3438            1.01m\n",
      "        93 60532831728089.9609           52.68s\n",
      "        94 60439614385497.3438           45.10s\n",
      "        95 59763445068519.9141           37.49s\n",
      "        96 59510105220503.8984           29.96s\n",
      "        97 58705291038160.7500           22.48s\n",
      "        98 58458452121494.5703           14.97s\n",
      "        99 57852223154081.5000            7.47s\n",
      "       100 56900722683908.7344            0.00s\n",
      "Validation Mean Squared Error: 166708205903752.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rf_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error', verbose=2)\n",
    "used.append('GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss=\\'squared_error\\')')\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "print(f'Validation Mean Squared Error: {val_mse}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({'row ID': test_data['row ID'], 'price_doc': predictions.flatten()})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "submission_df.to_csv('Day16.3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used = 87\n",
      "\n",
      "Removed sub_area\n",
      "OneHot Encoding\n",
      "Converted All Columns To float32\n",
      "P-Value Based Feature Selection\n",
      "log Normalization\n",
      "StandardScaler\n",
      "GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error')\n"
     ]
    }
   ],
   "source": [
    "print(\"Features Used = \" + str(features_used) + \"\\n\")\n",
    "for i in used:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
