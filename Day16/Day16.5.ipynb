{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop, Adagrad, Nadam, Ftrl\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test_data = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "\n",
    "used = []\n",
    "\n",
    "# Extract features and target variable\n",
    "X = train_data.drop('price_doc', axis=1)\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('sub_area', axis=1)\n",
    "X_test = X_test.drop('sub_area', axis=1)\n",
    "used.append('Removed sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(X_test) \n",
    "used.append('OneHot Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import LabelEncoder\n",
    "\n",
    "# categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# categorical_columns_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     X_test[column] = label_encoder.fit_transform(X_test[column])\n",
    "\n",
    "# used.append('Label Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all columns in X_train with dtypes object\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         X.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# # drop all columns in X_test with dtypes object\n",
    "# for col in X_test.columns:\n",
    "#     if X_test[col].dtype == 'object':\n",
    "#         X_test.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# used.append(\"Removed Object Dtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "# X_test = X_test[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "\n",
    "# used.append(\"Forward Feature Selection (n=14)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "# threshold_value = 4000000\n",
    "# variance_filter = VarianceThreshold(threshold=threshold_value)\n",
    "\n",
    "# X = variance_filter.fit_transform(X)\n",
    "# X_test = variance_filter.transform(X_test)\n",
    "\n",
    "# used.append(\"Variance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# # Add a constant term to the feature matrix\n",
    "# X_with_const = sm.add_constant(X)\n",
    "\n",
    "# # Fit a linear regression model\n",
    "# model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# # Get p-values for each feature\n",
    "# p_values = model.pvalues[1:]  # Exclude the constant term\n",
    "\n",
    "# # Set your desired threshold for p-value\n",
    "# threshold = 0.00001\n",
    "\n",
    "# # Filter features based on p-value\n",
    "# selected_features = p_values[p_values < threshold].index\n",
    "\n",
    "# # Display selected features\n",
    "# print(\"Selected Features:\")\n",
    "# print(selected_features)\n",
    "# print(len(selected_features))\n",
    "\n",
    "# # Select columns in the DataFrame\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"P-Value Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the Random Forest model\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, bootstrap=True)\n",
    "\n",
    "# # Train the model to get feature importances\n",
    "# rf_model.fit(X, y)\n",
    "\n",
    "# # Feature importance analysis\n",
    "# feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# # Create a DataFrame with feature names and their importance scores\n",
    "# feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# # Sort the features based on importance in descending order\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Select the top N most important features\n",
    "# top_n_features = 20  # You can choose the number of top features based on your preference\n",
    "# selected_features = feature_importance_df.head(top_n_features)['Feature'].tolist()\n",
    "\n",
    "# # Subset the original data to include only the selected features\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"Random Forest Feature Importance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "X = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "pca2 = PCA(n_components=10)\n",
    "principalComponents = pca2.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "used.append('PCA (n=10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.log1p(X)\n",
    "# X_test = np.log1p(X_test)\n",
    "# used.append('log Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "used.append(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127054, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)#, interaction_only=True)\n",
    "# X_train = poly.fit_transform(X_train)\n",
    "# X_test = poly.fit_transform(X_test)\n",
    "# used.append('PolynomialFeatures W/O Interaction')\n",
    "# # used.append('PolynomialFeatures With Interaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save X_train to csv\n",
    "# X_train.to_csv('train with poly w/o int.2.csv', index=False)\n",
    "# X_test.to_csv('test with poly w/o int.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=200)\n",
    "# principalComponents = pca.fit_transform(X_train)\n",
    "# X_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=200)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=200)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 416754634920680.6250            3.07m\n",
      "         2 368333187791698.0000            2.83m\n",
      "         3 328732277388635.1250            2.76m\n",
      "         4 295725362178829.9375            2.77m\n",
      "         5 269493870074958.8750            2.80m\n",
      "         6 247688698896542.7188            2.87m\n",
      "         7 229682567705139.2812            2.84m\n",
      "         8 214921887281954.1875            2.80m\n",
      "         9 202833899826069.6562            2.75m\n",
      "        10 193049379674421.5312            2.70m\n",
      "        11 184800258156148.0625            2.64m\n",
      "        12 177403996510277.5625            2.59m\n",
      "        13 171634593843474.4375            2.55m\n",
      "        14 166741319654094.3438            2.54m\n",
      "        15 162634109093846.2812            2.49m\n",
      "        16 158971053437050.3750            2.47m\n",
      "        17 155818295321609.9375            2.45m\n",
      "        18 152577161608213.2188            2.44m\n",
      "        19 148964558997675.5625            2.43m\n",
      "        20 146262397494334.5312            2.40m\n",
      "        21 144198984069986.5312            2.38m\n",
      "        22 141947244096377.7188            2.36m\n",
      "        23 139856731521521.5469            2.34m\n",
      "        24 137819440706335.7656            2.31m\n",
      "        25 136221923689713.6562            2.27m\n",
      "        26 135041746404658.8906            2.24m\n",
      "        27 133822205478338.7344            2.20m\n",
      "        28 132625808622635.3906            2.17m\n",
      "        29 131570546710635.2188            2.13m\n",
      "        30 129865465893124.2188            2.09m\n",
      "        31 128212894146793.8125            2.05m\n",
      "        32 126546698340900.8750            2.02m\n",
      "        33 125479023200748.9688            1.99m\n",
      "        34 124351466913448.7969            1.94m\n",
      "        35 122808264161870.0938            1.91m\n",
      "        36 121382262381773.1562            1.88m\n",
      "        37 119876154532408.3125            1.89m\n",
      "        38 119039564654276.1094            1.87m\n",
      "        39 118660229311571.3750            1.83m\n",
      "        40 117865078776854.9062            1.80m\n",
      "        41 116685775660930.1094            1.76m\n",
      "        42 115996857122596.8750            1.73m\n",
      "        43 114580047112705.5156            1.70m\n",
      "        44 113795515835716.7344            1.68m\n",
      "        45 112728099010422.8438            1.65m\n",
      "        46 112060203201377.5625            1.61m\n",
      "        47 111439024252979.6719            1.58m\n",
      "        48 110531466187790.7188            1.54m\n",
      "        49 109870899977260.8906            1.51m\n",
      "        50 108509506961952.4688            1.48m\n",
      "        51 107423229082235.5312            1.45m\n",
      "        52 106957083151785.7500            1.41m\n",
      "        53 106295151254344.9844            1.38m\n",
      "        54 105804090459027.5312            1.35m\n",
      "        55 105091749600781.4844            1.31m\n",
      "        56 104815470349791.9219            1.28m\n",
      "        57 104494417443046.8281            1.25m\n",
      "        58 104102881529111.8906            1.23m\n",
      "        59 103266405608044.8281            1.19m\n",
      "        60 103130578316542.2969            1.17m\n",
      "        61 102821970388464.1250            1.14m\n",
      "        62 102650216039216.5781            1.11m\n",
      "        63 102294019867965.9531            1.08m\n",
      "        64 101807143145439.7344            1.05m\n",
      "        65 101510807546220.0781            1.02m\n",
      "        66 101465455388620.8125           59.77s\n",
      "        67 100922303779680.3906           57.92s\n",
      "        68 100811263105473.9219           56.08s\n",
      "        69 100042571678890.8125           54.22s\n",
      "        70 99689141321090.8125           52.41s\n",
      "        71 99534816177278.2500           50.58s\n",
      "        72 99416002855823.7188           48.80s\n",
      "        73 98200793045129.8906           46.94s\n",
      "        74 97598512558262.7812           45.13s\n",
      "        75 97156068452863.4375           43.34s\n",
      "        76 96893727123712.2188           41.56s\n",
      "        77 96382367387074.1875           39.74s\n",
      "        78 95506179939323.6094           37.98s\n",
      "        79 94400997576398.3125           36.20s\n",
      "        80 93955469422760.2031           34.59s\n",
      "        81 93673805336262.4062           32.89s\n",
      "        82 93251604995692.6094           31.13s\n",
      "        83 92923846182822.1719           29.38s\n",
      "        84 92810175734132.5156           27.65s\n",
      "        85 91893469926913.7188           25.93s\n",
      "        86 91230293026573.3281           24.21s\n",
      "        87 90265412526840.9531           22.48s\n",
      "        88 89756611787513.7188           20.76s\n",
      "        89 89462624847391.1094           19.06s\n",
      "        90 88891443684165.7656           17.33s\n",
      "        91 88605835979733.4688           15.59s\n",
      "        92 88302137802247.0938           13.84s\n",
      "        93 87363483030612.2812           12.10s\n",
      "        94 86988230529008.0156           10.38s\n",
      "        95 86452038626232.3281            8.64s\n",
      "        96 86135157764433.6406            6.91s\n",
      "        97 85482124715606.9062            5.18s\n",
      "        98 85285099242163.4844            3.45s\n",
      "        99 85179123287910.5469            1.72s\n",
      "       100 84582222173757.4062            0.00s\n",
      "Validation Mean Squared Error: 176727441152463.47\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rf_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error', verbose=2)\n",
    "used.append('GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss=\\'squared_error\\')')\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "print(f'Validation Mean Squared Error: {val_mse}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({'row ID': test_data['row ID'], 'price_doc': predictions.flatten()})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "submission_df.to_csv('Day16.5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used = 10\n",
      "\n",
      "Removed sub_area\n",
      "OneHot Encoding\n",
      "PCA (n=10)\n",
      "StandardScaler\n",
      "GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Features Used = \" + str(features_used) + \"\\n\")\n",
    "for i in used:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
