{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop, Adagrad, Nadam, Ftrl\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test_data = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "\n",
    "used = []\n",
    "\n",
    "# Extract features and target variable\n",
    "X = train_data.drop('price_doc', axis=1)\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('sub_area', axis=1)\n",
    "X_test = X_test.drop('sub_area', axis=1)\n",
    "used.append('Removed sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(X_test) \n",
    "used.append('OneHot Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import LabelEncoder\n",
    "\n",
    "# categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# categorical_columns_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     X_test[column] = label_encoder.fit_transform(X_test[column])\n",
    "\n",
    "# used.append('Label Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all columns in X_train with dtypes object\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         X.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# # drop all columns in X_test with dtypes object\n",
    "# for col in X_test.columns:\n",
    "#     if X_test[col].dtype == 'object':\n",
    "#         X_test.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# used.append(\"Removed Object Dtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "# X_test = X_test[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    "#  'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    "#  'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "\n",
    "# used.append(\"Forward Feature Selection (n=14)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "# threshold_value = 4000000\n",
    "# variance_filter = VarianceThreshold(threshold=threshold_value)\n",
    "\n",
    "# X = variance_filter.fit_transform(X)\n",
    "# X_test = variance_filter.transform(X_test)\n",
    "\n",
    "# used.append(\"Variance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# # Add a constant term to the feature matrix\n",
    "# X_with_const = sm.add_constant(X)\n",
    "\n",
    "# # Fit a linear regression model\n",
    "# model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# # Get p-values for each feature\n",
    "# p_values = model.pvalues[1:]  # Exclude the constant term\n",
    "\n",
    "# # Set your desired threshold for p-value\n",
    "# threshold = 0.00001\n",
    "\n",
    "# # Filter features based on p-value\n",
    "# selected_features = p_values[p_values < threshold].index\n",
    "\n",
    "# # Display selected features\n",
    "# print(\"Selected Features:\")\n",
    "# print(selected_features)\n",
    "# print(len(selected_features))\n",
    "\n",
    "# # Select columns in the DataFrame\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"P-Value Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 415119387722197.3125           17.24m\n",
      "         2 364809917278342.2500           14.86m\n",
      "         3 324146418486510.6875           12.92m\n",
      "         4 290631658161203.1250           11.71m\n",
      "         5 263151698839354.8125           10.56m\n",
      "         6 241173886748647.9375            9.55m\n",
      "         7 222901692785901.3438            8.67m\n",
      "         8 207291936492291.4375            7.82m\n",
      "         9 194542517844857.0312            7.12m\n",
      "        10 184328232441338.9688            6.49m\n",
      "        11 175952942128201.4375            5.79m\n",
      "        12 167688159447018.0938            5.10m\n",
      "        13 161474942381453.2188            4.42m\n",
      "        14 155000892080855.8438            3.75m\n",
      "        15 150427678394866.6875            3.11m\n",
      "        16 145758372837330.1562            2.47m\n",
      "        17 142858808428190.6562            1.84m\n",
      "        18 139621163137247.9219            1.22m\n",
      "        19 137131414069764.5000           36.36s\n",
      "        20 134410183235053.2344            0.00s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rf_model = GradientBoostingRegressor(n_estimators=20, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error', verbose=2)\n",
    "used.append('GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss=\\'squared_error\\')')\n",
    "\n",
    "# Train the model to get feature importances\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the features based on importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top N most important features\n",
    "top_n_features = 10  # You can choose the number of top features based on your preference\n",
    "selected_features = feature_importance_df.head(top_n_features)['Feature'].tolist()\n",
    "\n",
    "# Subset the original data to include only the selected features\n",
    "X = X[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "used.append(\"GradientBoostingRegressor Feature Importance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=10)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=10)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.log1p(X)\n",
    "X_test = np.log1p(X_test)\n",
    "used.append('log Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "used.append(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127054, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)#, interaction_only=True)\n",
    "# X_train = poly.fit_transform(X_train)\n",
    "# X_test = poly.fit_transform(X_test)\n",
    "# used.append('PolynomialFeatures W/O Interaction')\n",
    "# # used.append('PolynomialFeatures With Interaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save X_train to csv\n",
    "# X_train.to_csv('train with poly w/o int.2.csv', index=False)\n",
    "# X_test.to_csv('test with poly w/o int.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=200)\n",
    "# principalComponents = pca.fit_transform(X_train)\n",
    "# X_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=200)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=200)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 415297968164954.9375            1.53m\n",
      "         2 365758662522569.3125            1.49m\n",
      "         3 325590516028060.7500            1.46m\n",
      "         4 292821300152088.0000            1.43m\n",
      "         5 266467976235713.7188            1.41m\n",
      "         6 244632586141804.3438            1.39m\n",
      "         7 227162493540404.3438            1.63m\n",
      "         8 212355496921519.6250            1.71m\n",
      "         9 200748321905361.1250            1.69m\n",
      "        10 191237535184521.9062            1.65m\n",
      "        11 183218997940455.3125            1.61m\n",
      "        12 176636649474690.6875            1.60m\n",
      "        13 171521420643193.0000            1.60m\n",
      "        14 167034734021593.7812            1.61m\n",
      "        15 162892112011471.7500            1.59m\n",
      "        16 159850834894818.8125            1.56m\n",
      "        17 156820598170511.8438            1.54m\n",
      "        18 154541012707439.4688            1.52m\n",
      "        19 152830610361361.6562            1.50m\n",
      "        20 150816867634987.3438            1.47m\n",
      "        21 149650508673488.9062            1.44m\n",
      "        22 148463691356135.5312            1.41m\n",
      "        23 146646581860722.6875            1.39m\n",
      "        24 145666339926183.3125            1.36m\n",
      "        25 145000950090557.1875            1.33m\n",
      "        26 143673067326673.2188            1.32m\n",
      "        27 143145083684799.0625            1.30m\n",
      "        28 142641670383568.9688            1.30m\n",
      "        29 141720369704668.5938            1.29m\n",
      "        30 141087225494594.6250            1.28m\n",
      "        31 140038825584267.3906            1.27m\n",
      "        32 139101327132984.1562            1.25m\n",
      "        33 137803181759744.3906            1.23m\n",
      "        34 136855792568301.9844            1.22m\n",
      "        35 135983173994417.2344            1.20m\n",
      "        36 134801948013534.6406            1.20m\n",
      "        37 133934281900187.8281            1.18m\n",
      "        38 132955871967123.4844            1.17m\n",
      "        39 132199000561273.0781            1.16m\n",
      "        40 131040832744852.3594            1.14m\n",
      "        41 130584514682385.7969            1.12m\n",
      "        42 129740702239008.0625            1.10m\n",
      "        43 128923186260750.1250            1.07m\n",
      "        44 128331485549450.8906            1.05m\n",
      "        45 128067113110498.7656            1.03m\n",
      "        46 126764911888264.6875            1.01m\n",
      "        47 125689009110898.1094           59.15s\n",
      "        48 125110006315204.9531           58.25s\n",
      "        49 124002438430811.2344           57.34s\n",
      "        50 123010706105217.1250           56.56s\n",
      "        51 121999435391588.9375           55.94s\n",
      "        52 121338055845675.9375           54.90s\n",
      "        53 120873947047700.6406           53.76s\n",
      "        54 119720844540821.5156           52.68s\n",
      "        55 119604119893972.5469           51.57s\n",
      "        56 119332996752104.7656           50.51s\n",
      "        57 118818001735807.3281           49.34s\n",
      "        58 118731022022310.3750           48.19s\n",
      "        59 117184640061948.8438           47.36s\n",
      "        60 116649895823382.6562           46.29s\n",
      "        61 115637452600988.0938           45.38s\n",
      "        62 115554029127764.1406           44.43s\n",
      "        63 114662485272333.4844           43.19s\n",
      "        64 114621438180106.1875           41.90s\n",
      "        65 114480530353007.4688           40.61s\n",
      "        66 114091450464250.6562           39.31s\n",
      "        67 113416302493496.8281           38.07s\n",
      "        68 113163361740281.8125           36.94s\n",
      "        69 112785309382758.0938           35.87s\n",
      "        70 112490946828488.2969           34.88s\n",
      "        71 112365013454758.7969           33.87s\n",
      "        72 111908507692778.2031           32.72s\n",
      "        73 111694600311974.4844           31.61s\n",
      "        74 111283249341421.4844           30.41s\n",
      "        75 110808135731144.9688           29.18s\n",
      "        76 110733893641046.9531           27.98s\n",
      "        77 110337060970158.3594           26.77s\n",
      "        78 110245518781442.1875           25.55s\n",
      "        79 109231375876891.3750           24.33s\n",
      "        80 108699054970975.7188           23.10s\n",
      "        81 107880770700504.6875           21.88s\n",
      "        82 107827762705475.1406           20.67s\n",
      "        83 107378903154122.5625           19.48s\n",
      "        84 106949516771652.6406           18.30s\n",
      "        85 106137613302203.0938           17.14s\n",
      "        86 104948787982550.9844           16.02s\n",
      "        87 104009457651355.2344           14.85s\n",
      "        88 103928238737102.6719           13.68s\n",
      "        89 103442633123550.8438           12.51s\n",
      "        90 102837761667132.2031           11.36s\n",
      "        91 102458922258487.3750           10.22s\n",
      "        92 102021488818559.1250            9.06s\n",
      "        93 101507624392576.7344            7.93s\n",
      "        94 101259816580767.5000            6.81s\n",
      "        95 101171289852413.3906            5.66s\n",
      "        96 100773619320631.8438            4.52s\n",
      "        97 100572345198236.5469            3.38s\n",
      "        98 100153630115147.0312            2.25s\n",
      "        99 99085518361877.5781            1.12s\n",
      "       100 99023617898938.1719            0.00s\n",
      "Validation Mean Squared Error: 165887061220037.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rf_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error', verbose=2)\n",
    "used.append('GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss=\\'squared_error\\')')\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "print(f'Validation Mean Squared Error: {val_mse}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({'row ID': test_data['row ID'], 'price_doc': predictions.flatten()})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "submission_df.to_csv('Day16.4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used = 10\n",
      "\n",
      "Removed sub_area\n",
      "OneHot Encoding\n",
      "GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error')\n",
      "GradientBoostingRegressor Feature Importance Based Feature Selection\n",
      "Converted All Columns To float32\n",
      "log Normalization\n",
      "StandardScaler\n",
      "GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error')\n"
     ]
    }
   ],
   "source": [
    "print(\"Features Used = \" + str(features_used) + \"\\n\")\n",
    "for i in used:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
