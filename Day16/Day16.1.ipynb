{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop, Adagrad, Nadam, Ftrl\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test_data = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "\n",
    "used = []\n",
    "\n",
    "# Extract features and target variable\n",
    "X = train_data.drop('price_doc', axis=1)\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.drop('sub_area', axis=1)\n",
    "# X_test = X_test.drop('sub_area', axis=1)\n",
    "# used.append('Removed sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(X_test) \n",
    "used.append('OneHot Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import LabelEncoder\n",
    "\n",
    "# categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# categorical_columns_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     X_test[column] = label_encoder.fit_transform(X_test[column])\n",
    "\n",
    "# used.append('Label Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all columns in X_train with dtypes object\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         X.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# # drop all columns in X_test with dtypes object\n",
    "# for col in X_test.columns:\n",
    "#     if X_test[col].dtype == 'object':\n",
    "#         X_test.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# used.append(\"Removed Object Dtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    " 'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    " 'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "X_test = X_test[['full_sq', 'floor', 'build_count_monolith', 'industrial_km', 'trc_sqm_500',\n",
    " 'mosque_count_500', 'leisure_count_500', 'office_sqm_1000',\n",
    " 'cafe_count_1000_price_high', 'leisure_count_1000', 'power_transmission_line_km', 'big_market_km', 'public_healthcare_km', 'workplaces_km']]\n",
    "\n",
    "used.append(\"Forward Feature Selection (n=14)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "# threshold_value = 4000000\n",
    "# variance_filter = VarianceThreshold(threshold=threshold_value)\n",
    "\n",
    "# X = variance_filter.fit_transform(X)\n",
    "# X_test = variance_filter.transform(X_test)\n",
    "\n",
    "# used.append(\"Variance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# # Add a constant term to the feature matrix\n",
    "# X_with_const = sm.add_constant(X)\n",
    "\n",
    "# # Fit a linear regression model\n",
    "# model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# # Get p-values for each feature\n",
    "# p_values = model.pvalues[1:]  # Exclude the constant term\n",
    "\n",
    "# # Set your desired threshold for p-value\n",
    "# threshold = 0.00001\n",
    "\n",
    "# # Filter features based on p-value\n",
    "# selected_features = p_values[p_values < threshold].index\n",
    "\n",
    "# # Display selected features\n",
    "# print(\"Selected Features:\")\n",
    "# print(selected_features)\n",
    "# print(len(selected_features))\n",
    "\n",
    "# # Select columns in the DataFrame\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"P-Value Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the Random Forest model\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, bootstrap=True)\n",
    "\n",
    "# # Train the model to get feature importances\n",
    "# rf_model.fit(X, y)\n",
    "\n",
    "# # Feature importance analysis\n",
    "# feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# # Create a DataFrame with feature names and their importance scores\n",
    "# feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# # Sort the features based on importance in descending order\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Select the top N most important features\n",
    "# top_n_features = 20  # You can choose the number of top features based on your preference\n",
    "# selected_features = feature_importance_df.head(top_n_features)['Feature'].tolist()\n",
    "\n",
    "# # Subset the original data to include only the selected features\n",
    "# X = X[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "\n",
    "# used.append(\"Random Forest Feature Importance Based Feature Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=10)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=10)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.log1p(X)\n",
    "X_test = np.log1p(X_test)\n",
    "used.append('log Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "used.append(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127054, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)#, interaction_only=True)\n",
    "# X_train = poly.fit_transform(X_train)\n",
    "# X_test = poly.fit_transform(X_test)\n",
    "# used.append('PolynomialFeatures W/O Interaction')\n",
    "# # used.append('PolynomialFeatures With Interaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save X_train to csv\n",
    "# X_train.to_csv('train with poly w/o int.2.csv', index=False)\n",
    "# X_test.to_csv('test with poly w/o int.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=200)\n",
    "# principalComponents = pca.fit_transform(X_train)\n",
    "# X_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=200)\n",
    "# principalComponents = pca2.fit_transform(X_test)\n",
    "# X_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# used.append('PCA (n=200)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 415777936577866.0625            4.24m\n",
      "         2 366730145636191.5625            3.47m\n",
      "         3 326971880795533.0000            3.41m\n",
      "         4 294614913177324.5625            3.18m\n",
      "         5 268011488317943.6562            3.05m\n",
      "         6 246438649670620.9062            2.99m\n",
      "         7 229089108402188.2188            2.90m\n",
      "         8 214871740999707.8125            2.83m\n",
      "         9 203297980608884.4062            2.75m\n",
      "        10 193482199379962.7812            2.67m\n",
      "        11 185307295937341.0000            2.61m\n",
      "        12 178765003800628.5312            2.57m\n",
      "        13 173035999680406.3125            2.51m\n",
      "        14 168780505994943.7812            2.46m\n",
      "        15 165011263169720.6250            2.41m\n",
      "        16 161357639171964.3125            2.36m\n",
      "        17 158581680762206.9062            2.32m\n",
      "        18 155921572814387.1875            2.28m\n",
      "        19 154072078871757.9375            2.25m\n",
      "        20 152196918585190.7188            2.22m\n",
      "        21 150351309967016.1250            2.19m\n",
      "        22 148700674668628.6562            2.16m\n",
      "        23 147215986775287.6562            2.13m\n",
      "        24 145252415791024.1875            2.09m\n",
      "        25 143762063319014.4375            2.06m\n",
      "        26 142264221531256.4688            2.03m\n",
      "        27 140696122161898.7031            1.99m\n",
      "        28 139575094569998.0781            1.96m\n",
      "        29 138793736578330.7188            1.93m\n",
      "        30 138391434441276.0312            1.90m\n",
      "        31 137099358824896.2812            1.87m\n",
      "        32 134858071824290.3125            1.84m\n",
      "        33 134410671455620.5781            1.81m\n",
      "        34 132836095723130.4688            1.80m\n",
      "        35 131366661467916.4688            1.78m\n",
      "        36 131155818107770.9844            1.76m\n",
      "        37 129793760866829.2812            1.73m\n",
      "        38 129040227215569.3281            1.70m\n",
      "        39 128012096662171.1406            1.67m\n",
      "        40 127103373182316.3281            1.65m\n",
      "        41 125831669774641.7188            1.64m\n",
      "        42 125234795199604.2500            1.62m\n",
      "        43 124110265611412.1094            1.61m\n",
      "        44 123392001961762.0000            1.59m\n",
      "        45 121719932183093.0938            1.57m\n",
      "        46 120215881044451.6250            1.55m\n",
      "        47 119266191081569.2031            1.53m\n",
      "        48 117451469972544.5625            1.50m\n",
      "        49 116566812218136.8438            1.48m\n",
      "        50 115381161107605.2188            1.45m\n",
      "        51 114334952937020.9688            1.43m\n",
      "        52 113521830787887.6406            1.40m\n",
      "        53 112460012313591.2812            1.39m\n",
      "        54 111488815704540.9375            1.36m\n",
      "        55 111326267435207.9062            1.33m\n",
      "        56 110429158939064.6094            1.31m\n",
      "        57 110216125580061.4844            1.29m\n",
      "        58 109364600635788.4375            1.26m\n",
      "        59 109263229934275.7344            1.23m\n",
      "        60 109168201694924.1250            1.20m\n",
      "        61 109038937965899.9219            1.17m\n",
      "        62 107893191202168.7969            1.14m\n",
      "        63 107435311494367.6250            1.12m\n",
      "        64 106860366037746.1406            1.09m\n",
      "        65 106758540732857.4844            1.06m\n",
      "        66 105896982304912.8125            1.03m\n",
      "        67 105838178771766.6562           59.93s\n",
      "        68 105760286156371.6094           58.15s\n",
      "        69 105656697454911.2656           56.34s\n",
      "        70 105564931608746.1875           54.52s\n",
      "        71 105348815248604.3750           52.70s\n",
      "        72 104695607730900.5469           50.89s\n",
      "        73 104155873407792.8125           49.09s\n",
      "        74 104068400735967.0469           47.30s\n",
      "        75 103979547314317.1250           45.55s\n",
      "        76 103772296169460.8281           43.77s\n",
      "        77 103576462183122.0312           41.98s\n",
      "        78 102898384110135.5312           40.19s\n",
      "        79 101475813062212.1250           38.39s\n",
      "        80 100855928221734.6562           36.62s\n",
      "        81 99941649185293.9375           34.78s\n",
      "        82 99531635269116.1719           32.93s\n",
      "        83 99088582265985.6094           31.09s\n",
      "        84 98911029060283.6562           29.24s\n",
      "        85 98658086881600.4062           27.42s\n",
      "        86 98527769059547.5625           25.65s\n",
      "        87 97277374727231.2656           23.86s\n",
      "        88 96387962481032.4688           22.02s\n",
      "        89 95516314220316.6875           20.19s\n",
      "        90 94504512292472.9219           18.36s\n",
      "        91 93895866595120.2969           16.54s\n",
      "        92 93800975858271.8750           14.72s\n",
      "        93 93721429603678.2031           12.92s\n",
      "        94 93650418790964.1875           11.08s\n",
      "        95 93604606528542.0938            9.24s\n",
      "        96 92940063907470.2812            7.39s\n",
      "        97 92844257402404.0469            5.54s\n",
      "        98 91999587155437.3906            3.69s\n",
      "        99 91368525808309.2344            1.84s\n",
      "       100 91280660178363.2500            0.00s\n",
      "Validation Mean Squared Error: 166339084147197.44\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rf_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error', verbose=2)\n",
    "used.append('GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss=\\'squared_error\\')')\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "print(f'Validation Mean Squared Error: {val_mse}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({'row ID': test_data['row ID'], 'price_doc': predictions.flatten()})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "submission_df.to_csv('Day16.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used = 14\n",
      "\n",
      "OneHot Encoding\n",
      "Forward Feature Selection (n=14)\n",
      "log Normalization\n",
      "StandardScaler\n",
      "GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42, loss='squared_error')\n"
     ]
    }
   ],
   "source": [
    "print(\"Features Used = \" + str(features_used) + \"\\n\")\n",
    "for i in used:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
