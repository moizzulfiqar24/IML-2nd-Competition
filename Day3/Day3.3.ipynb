{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "testOriginal = pd.read_csv('../../2nd-Comp-Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 272)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('row ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SubArea Removal<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('sub_area', axis=1, inplace=True)\n",
    "train.drop('sub_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Categorical To Numerical<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>OneHot<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.get_dummies(train)\n",
    "# test = pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Label<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Categorical columns: ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion', 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', 'water_1line', 'big_road1_1line', 'railroad_1line', 'ecology']\n",
      "Test: Categorical columns: ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion', 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', 'water_1line', 'big_road1_1line', 'railroad_1line', 'ecology']\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    train[column] = label_encoder.fit_transform(train[column])\n",
    "\n",
    "categorical_columns_test = test.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns_test:\n",
    "    test[column] = label_encoder.fit_transform(test[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scaling<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_scaling_needed(dataframe, threshold=5):\n",
    "#     \"\"\"\n",
    "#     Identify columns in a pandas DataFrame that may require scaling.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Range threshold to determine if a column requires scaling (default is 5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, range_value) for columns that may require scaling.\n",
    "#     \"\"\"\n",
    "#     scaling_needed_columns = []\n",
    "\n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             column_range = dataframe[column].max() - dataframe[column].min()\n",
    "#             if column_range > threshold:\n",
    "#                 scaling_needed_columns.append((column, column_range))\n",
    "\n",
    "#     return scaling_needed_columns\n",
    "\n",
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "\n",
    "# if not scaling_needed_columns:\n",
    "#     print(\"No columns require scaling.\")\n",
    "# else:\n",
    "#     print(\"Columns that may require scaling:\")\n",
    "#     for column, column_range in scaling_needed_columns:\n",
    "#         print(f\"{column}: Range = {column_range}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# for column, _ in scaling_needed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = scaler.fit_transform(train[[column]])\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "# train = scaler.fit_transform(train)\n",
    "# test = scaler.fit_transform(test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_skewed_columns(dataframe, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Identify skewed columns in a pandas DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Skewness threshold to determine if a column is skewed (default is 0.5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, skewness_value) for skewed columns.\n",
    "#     \"\"\"\n",
    "#     skewed_columns = []\n",
    "    \n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             skewness = skew(dataframe[column])\n",
    "#             if abs(skewness) > threshold:\n",
    "#                 skewed_columns.append((column, skewness))\n",
    "    \n",
    "#     return skewed_columns\n",
    "\n",
    "# skewed_columns = find_skewed_columns(train)\n",
    "\n",
    "# if not skewed_columns:\n",
    "#     print(\"No skewed columns found.\")\n",
    "# else:\n",
    "#     print(\"Skewed columns:\")\n",
    "#     for column, skewness in skewed_columns:\n",
    "#         print(f\"{column}: Skewness = {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column, _ in skewed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = train[column].apply(lambda x: 1 if x == 0 else np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.loc[:, train.columns != 'price_doc']\n",
    "y = train[['price_doc']]\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# test = scaler.transform(test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 270)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 270)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PCA<h3> \n",
    "<h5><i>Remember to set X2 & test2<i><h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=20)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X2 = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=20)\n",
    "# principalComponents = pca2.fit_transform(test)\n",
    "# test2 = pd.DataFrame(data = principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot an elbow graph to find the optimal number of components\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(pca.explained_variance_ratio_)\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=5)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]\n",
    "# test = test[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Importance<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #give me code for feature importance using linear regression\n",
    "# model = LinearRegression()\n",
    "# model.fit(X, y)\n",
    "# # get importance\n",
    "# importance = model.coef_[0]\n",
    "# # summarize feature importance\n",
    "# for i,v in enumerate(importance):\n",
    "#     print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# # drop features with low importance\n",
    "# threshold = 40000  # Adjust the threshold as needed\n",
    "# selected_features = [feature for feature, score in zip(X.columns, importance) if score >= threshold]\n",
    "# print(len(selected_features))\n",
    "# X = X[selected_features]\n",
    "# test = test[selected_features]\n",
    "\n",
    "# # plot feature importance\n",
    "# plt.bar([x for x in range(len(importance))], importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Feature Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=12)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X[['full_sq', 'floor', 'build_count_monolith' ,'industrial_km', 'prom_part_500','trc_sqm_500', 'mosque_count_500', 'leisure_count_500' ,'office_sqm_1000','cafe_count_1000_price_high', 'leisure_count_1000', 'market_count_1000']]\n",
    "test2 = test[['full_sq', 'floor', 'build_count_monolith' ,'industrial_km', 'prom_part_500','trc_sqm_500', 'mosque_count_500', 'leisure_count_500' ,'office_sqm_1000','cafe_count_1000_price_high', 'leisure_count_1000', 'market_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Robust Scaling<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X2 = scaler.fit_transform(X2)\n",
    "test2 = scaler.transform(test2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization For Robust<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2 = np.log(X2)\n",
    "# test2 = np.log(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_normalizer = PowerTransformer(method='box-cox', standardize=False)\n",
    "# X2 = log_normalizer.fit_transform(X2)\n",
    "# test2 = log_normalizer.transform(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_skewed_columns(dataframe, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Identify skewed columns in a pandas DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Skewness threshold to determine if a column is skewed (default is 0.5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, skewness_value) for skewed columns.\n",
    "#     \"\"\"\n",
    "#     skewed_columns = []\n",
    "    \n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             skewness = skew(dataframe[column])\n",
    "#             if abs(skewness) > threshold:\n",
    "#                 skewed_columns.append((column, skewness))\n",
    "    \n",
    "#     return skewed_columns\n",
    "\n",
    "# skewed_columns = find_skewed_columns(X2)\n",
    "\n",
    "# if not skewed_columns:\n",
    "#     print(\"No skewed columns found.\")\n",
    "# else:\n",
    "#     print(\"Skewed columns:\")\n",
    "#     for column, skewness in skewed_columns:\n",
    "#         print(f\"{column}: Skewness = {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column, _ in skewed_columns:\n",
    "#     if X2[column].dtype in ['int64', 'float64']:\n",
    "#         X2[column] = X2[column].apply(lambda x: 1 if x == 0 else np.log(x))\n",
    "#         test2[column] = test2[column].apply(lambda x: 1 if x == 0 else np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>P-Value Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Convert scaled_df1 to a DataFrame without specifying columns\n",
    "# # scaled_df1_df = pd.DataFrame(scaled_df1)\n",
    "\n",
    "# X_p = X.astype(float)\n",
    "\n",
    "# # Add a constant term to the feature matrix\n",
    "# X_with_const = sm.add_constant(X_p)\n",
    "\n",
    "# # Fit a linear regression model\n",
    "# model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# # Get p-values for each feature\n",
    "# p_values = model.pvalues[1:]  # Exclude the constant term\n",
    "\n",
    "# # Set your desired threshold for p-value\n",
    "# threshold = 0.00001\n",
    "\n",
    "# # Filter features based on p-value\n",
    "# selected_features = p_values[p_values < threshold].index\n",
    "\n",
    "# # Display selected features\n",
    "# print(\"Selected Features:\")\n",
    "# print(selected_features)\n",
    "# print(len(selected_features))\n",
    "\n",
    "# # Select columns in the DataFrame\n",
    "# X2 = X[selected_features]\n",
    "# test2 = test[selected_features]\n",
    "\n",
    "# # X = X.astype(float)\n",
    "# # X = sm.add_constant(X)\n",
    "# # mod = sm.OLS(y, X)\n",
    "# # res = mod.fit()\n",
    "# # pvalues = res.pvalues\n",
    "# # columns = pvalues[pvalues<0.05].index\n",
    "# # print(columns)\n",
    "# # print(len(columns))\n",
    "\n",
    "# # # Select columns in the DataFrame\n",
    "# # X2 = X[columns]\n",
    "# # test2 = test[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Variance Based Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_value = 100000\n",
    "# variance_filter = VarianceThreshold(threshold=threshold_value)\n",
    "\n",
    "# X2 = variance_filter.fit_transform(X)\n",
    "# test2 = variance_filter.transform(test)\n",
    "\n",
    "# X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Poly Interaction ONNNNNNNNN<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, interaction_only=True)\n",
    "X3 = poly.fit_transform(X2)\n",
    "test3 = poly.fit_transform(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applying Model<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 79)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 79)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.05721845e-10  4.82068888e+05  2.22138853e+06  6.17075400e+05\n",
      "   9.19128354e+05  5.37283097e+05  2.25893017e+05  9.54178047e+06\n",
      "   2.07280902e+06  8.88651899e+05  1.64724126e+06  1.21840830e+05\n",
      "   6.38087160e+05 -1.12897561e+04 -7.85281826e+03 -6.12940810e+03\n",
      "  -1.41461285e+04 -2.41064055e+03 -9.59896440e+04 -1.25153354e+04\n",
      "  -5.88083711e+03 -9.77844127e+03 -2.28998609e+03 -1.49954668e+04\n",
      "   1.59584272e+04 -1.41702620e+04 -1.24419532e+05 -5.73348212e+03\n",
      "  -4.52335855e+05 -4.31573801e+04 -3.66603334e+04 -5.42570911e+03\n",
      "  -3.19204042e+03 -2.42657376e+04  3.09928048e+04 -1.87936216e+04\n",
      "  -5.06818286e+03  6.40346530e+04  8.51752802e+03 -4.59347218e+04\n",
      "  -1.72997099e+03  5.43591731e+03  3.40990755e+04 -1.68303035e+04\n",
      "  -4.26506945e+03 -1.99420682e+05 -2.14403984e+04  1.01595162e+04\n",
      "  -9.60551002e+03 -9.91479953e+03  5.13826009e+04  1.64282098e+04\n",
      "   2.02913126e+05  3.80887200e+04  6.84108904e+04  4.87950501e+04\n",
      "   9.65987899e+03  5.61105949e+04  3.59088758e+04 -5.90586662e+03\n",
      "  -3.83125687e+03  5.18284593e+03 -2.81209664e+02 -9.39134049e+02\n",
      "  -1.02053333e+05  1.88384504e+04 -1.83665667e+05  1.46612028e+05\n",
      "  -1.70865083e+05 -2.06241235e+04 -7.53774178e+04  4.55113267e+03\n",
      "  -7.09775365e+04 -2.91651503e+04  1.65315805e+03  1.82517219e+03\n",
      "  -2.05449575e+04  2.61250934e+04  3.82902465e+04]]\n",
      "[6036528.11426234]\n"
     ]
    }
   ],
   "source": [
    "reg2 = LinearRegression().fit(X3, y)\n",
    "print(reg2.coef_)\n",
    "print(reg2.intercept_)\n",
    "y_pred = reg2.predict(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'row ID': testOriginal['row ID'], 'price_doc': y_pred.flatten()})\n",
    "result_df.to_csv('Day3.3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
