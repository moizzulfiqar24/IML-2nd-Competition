{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "testOriginal = pd.read_csv('../../2nd-Comp-Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 272)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('row ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SubArea Removal<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('sub_area', axis=1, inplace=True)\n",
    "train.drop('sub_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Categorical To Numerical<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>OneHot<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Label<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     train[column] = label_encoder.fit_transform(train[column])\n",
    "\n",
    "# categorical_columns_test = test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     test[column] = label_encoder.fit_transform(test[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scaling<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_scaling_needed(dataframe, threshold=5):\n",
    "#     \"\"\"\n",
    "#     Identify columns in a pandas DataFrame that may require scaling.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Range threshold to determine if a column requires scaling (default is 5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, range_value) for columns that may require scaling.\n",
    "#     \"\"\"\n",
    "#     scaling_needed_columns = []\n",
    "\n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             column_range = dataframe[column].max() - dataframe[column].min()\n",
    "#             if column_range > threshold:\n",
    "#                 scaling_needed_columns.append((column, column_range))\n",
    "\n",
    "#     return scaling_needed_columns\n",
    "\n",
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "\n",
    "# if not scaling_needed_columns:\n",
    "#     print(\"No columns require scaling.\")\n",
    "# else:\n",
    "#     print(\"Columns that may require scaling:\")\n",
    "#     for column, column_range in scaling_needed_columns:\n",
    "#         print(f\"{column}: Range = {column_range}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# for column, _ in scaling_needed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = scaler.fit_transform(train[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_skewed_columns(dataframe, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Identify skewed columns in a pandas DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Skewness threshold to determine if a column is skewed (default is 0.5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, skewness_value) for skewed columns.\n",
    "#     \"\"\"\n",
    "#     skewed_columns = []\n",
    "    \n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             skewness = skew(dataframe[column])\n",
    "#             if abs(skewness) > threshold:\n",
    "#                 skewed_columns.append((column, skewness))\n",
    "    \n",
    "#     return skewed_columns\n",
    "\n",
    "# skewed_columns = find_skewed_columns(train)\n",
    "\n",
    "# if not skewed_columns:\n",
    "#     print(\"No skewed columns found.\")\n",
    "# else:\n",
    "#     print(\"Skewed columns:\")\n",
    "#     for column, skewness in skewed_columns:\n",
    "#         print(f\"{column}: Skewness = {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column, _ in skewed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = train[column].apply(lambda x: 1 if x == 0 else np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.loc[:, train.columns != 'price_doc']\n",
    "y = train[['price_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 287)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 287)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PCA<h3> \n",
    "<h5><i>Remember to set X2 & test2<i><h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=287)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X2 = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=287)\n",
    "# principalComponents = pca2.fit_transform(test)\n",
    "# test2 = pd.DataFrame(data = principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot an elbow graph to find the optimal number of components\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(pca.explained_variance_ratio_)\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=5)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]\n",
    "# test = test[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Feature Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=15)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2 = X[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]\n",
    "# test2 = test[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>P-Value Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_significant_columns(df, target_variable_name):\n",
    "#     # Initialize lists to store column names and corresponding p-values\n",
    "#     significant_columns = []\n",
    "#     p_values = []\n",
    "\n",
    "#     # Loop through each column in the DataFrame\n",
    "#     for column in df.columns:\n",
    "#         # Skip the target variable itself\n",
    "#         if column == target_variable_name:\n",
    "#             continue\n",
    "        \n",
    "#         # Add a constant term for the intercept\n",
    "#         X = sm.add_constant(df[column])\n",
    "        \n",
    "#         # Fit the linear regression model\n",
    "#         model = sm.OLS(df[target_variable_name], X).fit()\n",
    "        \n",
    "#         # Get the p-value for the coefficient of the predictor variable\n",
    "#         p_value = model.pvalues[1]\n",
    "        \n",
    "#         # Print information about the current column\n",
    "#         print(f\"{column}: p-value = {p_value}\")\n",
    "        \n",
    "#         # Check if the p-value is less than or equal to 0.05\n",
    "#         if p_value <= 0.05:\n",
    "#             significant_columns.append(column)\n",
    "#             p_values.append(p_value)\n",
    "\n",
    "#     # Print total significant columns\n",
    "#     print(\"\\nTotal columns with p-value <= 0.05:\", len(significant_columns))\n",
    "    \n",
    "#     # Print names of significant columns\n",
    "#     print(\"\\nColumns with p-value <= 0.05:\", significant_columns)\n",
    "    \n",
    "#     # Create a new DataFrame with only the significant columns\n",
    "#     df_significant = df[significant_columns]\n",
    "    \n",
    "#     return df_significant\n",
    "\n",
    "# # Example usage:\n",
    "# target_variable_name = 'price_doc'\n",
    "# X2 = find_significant_columns(train, target_variable_name)\n",
    "# print(\"\\nDataFrame with significant columns:\")\n",
    "# print(X2)\n",
    "\n",
    "# test2 = test[X2.columns]\n",
    "\n",
    "# X = X.astype(float)\n",
    "# X = sm.add_constant(X)\n",
    "# mod = sm.OLS(y, X)\n",
    "# res = mod.fit()\n",
    "# print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3 = X\n",
    "\n",
    "# def find_significant_columns(X, y):\n",
    "#     # Convert the entire DataFrame to float\n",
    "#     X = X.astype(float)\n",
    "\n",
    "#     significant_columns = []\n",
    "\n",
    "#     for column in X.columns:\n",
    "#         if X[column].dtype == float:  # Only consider float columns\n",
    "#             X_with_constant = sm.add_constant(X[column])\n",
    "#             model = sm.OLS(y, X_with_constant).fit()\n",
    "#             p_value = model.pvalues[1]  # Extract the p-value for the independent variable\n",
    "\n",
    "#             if p_value <= 0.05:\n",
    "#                 significant_columns.append(column)\n",
    "#                 print(f\"{column}: p-value = {p_value:.4f}\")\n",
    "\n",
    "#     print(\"\\nTotal columns with p-value <= 0.05:\", len(significant_columns))\n",
    "\n",
    "#     # Create a new DataFrame with only significant columns\n",
    "#     significant_df = X[significant_columns]\n",
    "\n",
    "#     return significant_df\n",
    "\n",
    "# X2 = find_significant_columns(X, y)\n",
    "# print(\"\\nDataFrame with significant columns:\")\n",
    "# print(X2)\n",
    "\n",
    "# # print columns in X but not in X2\n",
    "# print(\"\\nColumns in X but not in X2:\")\n",
    "# print(set(X3.columns) - set(X2.columns))\n",
    "\n",
    "# test2 = test[X2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print columns in X but not in X2\n",
    "# print(\"\\nColumns in X but not in X2:\")\n",
    "# print(set(X3.columns) - set(X2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X = X.astype(float)\n",
    "# # X = sm.add_constant(X)\n",
    "# # mod = sm.OLS(y, X)\n",
    "# # res = mod.fit()\n",
    "# # print(res.summary())\n",
    "\n",
    "# X = X.astype(float)\n",
    "# X = sm.add_constant(X)\n",
    "# mod = sm.OLS(y, X)\n",
    "# res = mod.fit()\n",
    "\n",
    "# file_name = \"output_summary.txt\"\n",
    "\n",
    "# with open(file_name, 'w') as file:\n",
    "#     print(res.summary(), file=file)\n",
    "\n",
    "# print(f\"Summary saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def find_significant_columns(file_path):\n",
    "#     # Read the file into a DataFrame\n",
    "#     df = pd.read_table(file_path, skiprows=63)  # Skip the initial rows until the column names\n",
    "\n",
    "#     # Print column names for inspection\n",
    "#     print(\"Column Names:\")\n",
    "#     print(df.columns)\n",
    "\n",
    "#     # Replace 'P>|t|' with the correct column name\n",
    "#     # Extract the column names and p-values\n",
    "#     column_names = df.columns\n",
    "#     p_values = df['Replace_With_Correct_Column_Name']\n",
    "\n",
    "#     # Find columns where p < 0.05\n",
    "#     significant_columns = column_names[p_values < 0.05]\n",
    "\n",
    "#     return significant_columns\n",
    "\n",
    "# # Replace 'output_summary.txt' with the actual file path\n",
    "# file_path = 'output_summary.txt'\n",
    "# significant_columns = find_significant_columns(file_path)\n",
    "\n",
    "# # Print or use the significant columns as needed\n",
    "# print(\"Significant Columns:\")\n",
    "# print(significant_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PolyNomial Features W/O Interaction<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th - Fall 2023/IML/IML-2nd-Competition/Day3/Day3.1.ipynb Cell 39\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day3/Day3.1.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m poly \u001b[39m=\u001b[39m PolynomialFeatures(\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day3/Day3.1.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X2 \u001b[39m=\u001b[39m poly\u001b[39m.\u001b[39;49mfit_transform(X)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day3/Day3.1.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(X2\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day3/Day3.1.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(poly\u001b[39m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_polynomial.py:554\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[39m# XP[:, start:end] are terms of degree d - 1\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[39m# that exclude feature #feature_idx.\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m     np\u001b[39m.\u001b[39;49mmultiply(\n\u001b[1;32m    555\u001b[0m         XP[:, start:end],\n\u001b[1;32m    556\u001b[0m         X[:, feature_idx : feature_idx \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m],\n\u001b[1;32m    557\u001b[0m         out\u001b[39m=\u001b[39;49mXP[:, current_col:next_col],\n\u001b[1;32m    558\u001b[0m         casting\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mno\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    559\u001b[0m     )\n\u001b[1;32m    560\u001b[0m     current_col \u001b[39m=\u001b[39m next_col\n\u001b[1;32m    562\u001b[0m new_index\u001b[39m.\u001b[39mappend(current_col)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2.shape)\n",
    "print(poly.get_feature_names_out())\n",
    "test2 = poly.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applying Model<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 286)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 286)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = LinearRegression().fit(X2, y)\n",
    "print(reg2.coef_)\n",
    "print(reg2.intercept_)\n",
    "y_pred = reg2.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'row ID': testOriginal['row ID'], 'price_doc': y_pred.flatten()})\n",
    "result_df.to_csv('Day3.1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
