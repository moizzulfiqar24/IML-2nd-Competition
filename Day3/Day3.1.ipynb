{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "testOriginal = pd.read_csv('../../2nd-Comp-Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 272)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('row ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SubArea Removal<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('sub_area', axis=1, inplace=True)\n",
    "train.drop('sub_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Categorical To Numerical<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>OneHot<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Label<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     train[column] = label_encoder.fit_transform(train[column])\n",
    "\n",
    "# categorical_columns_test = test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     test[column] = label_encoder.fit_transform(test[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scaling<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_scaling_needed(dataframe, threshold=5):\n",
    "#     \"\"\"\n",
    "#     Identify columns in a pandas DataFrame that may require scaling.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Range threshold to determine if a column requires scaling (default is 5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, range_value) for columns that may require scaling.\n",
    "#     \"\"\"\n",
    "#     scaling_needed_columns = []\n",
    "\n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             column_range = dataframe[column].max() - dataframe[column].min()\n",
    "#             if column_range > threshold:\n",
    "#                 scaling_needed_columns.append((column, column_range))\n",
    "\n",
    "#     return scaling_needed_columns\n",
    "\n",
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "\n",
    "# if not scaling_needed_columns:\n",
    "#     print(\"No columns require scaling.\")\n",
    "# else:\n",
    "#     print(\"Columns that may require scaling:\")\n",
    "#     for column, column_range in scaling_needed_columns:\n",
    "#         print(f\"{column}: Range = {column_range}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# for column, _ in scaling_needed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = scaler.fit_transform(train[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_skewed_columns(dataframe, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Identify skewed columns in a pandas DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Skewness threshold to determine if a column is skewed (default is 0.5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, skewness_value) for skewed columns.\n",
    "#     \"\"\"\n",
    "#     skewed_columns = []\n",
    "    \n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             skewness = skew(dataframe[column])\n",
    "#             if abs(skewness) > threshold:\n",
    "#                 skewed_columns.append((column, skewness))\n",
    "    \n",
    "#     return skewed_columns\n",
    "\n",
    "# skewed_columns = find_skewed_columns(train)\n",
    "\n",
    "# if not skewed_columns:\n",
    "#     print(\"No skewed columns found.\")\n",
    "# else:\n",
    "#     print(\"Skewed columns:\")\n",
    "#     for column, skewness in skewed_columns:\n",
    "#         print(f\"{column}: Skewness = {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column, _ in skewed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = train[column].apply(lambda x: 1 if x == 0 else np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.loc[:, train.columns != 'price_doc']\n",
    "y = train[['price_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 270)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 270)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PCA<h3> \n",
    "<h5><i>Remember to set X2 & test2<i><h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=20)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X2 = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=20)\n",
    "# principalComponents = pca2.fit_transform(test)\n",
    "# test2 = pd.DataFrame(data = principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot an elbow graph to find the optimal number of components\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(pca.explained_variance_ratio_)\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=5)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]\n",
    "# test = test[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Feature Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=15)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2 = X[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]\n",
    "# test2 = test[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>P-Value Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_significant_columns(df, target_variable_name):\n",
    "#     # Initialize lists to store column names and corresponding p-values\n",
    "#     significant_columns = []\n",
    "#     p_values = []\n",
    "\n",
    "#     # Loop through each column in the DataFrame\n",
    "#     for column in df.columns:\n",
    "#         # Skip the target variable itself\n",
    "#         if column == target_variable_name:\n",
    "#             continue\n",
    "        \n",
    "#         # Add a constant term for the intercept\n",
    "#         X = sm.add_constant(df[column])\n",
    "        \n",
    "#         # Fit the linear regression model\n",
    "#         model = sm.OLS(df[target_variable_name], X).fit()\n",
    "        \n",
    "#         # Get the p-value for the coefficient of the predictor variable\n",
    "#         p_value = model.pvalues[1]\n",
    "        \n",
    "#         # Print information about the current column\n",
    "#         print(f\"{column}: p-value = {p_value}\")\n",
    "        \n",
    "#         # Check if the p-value is less than or equal to 0.05\n",
    "#         if p_value <= 0.05:\n",
    "#             significant_columns.append(column)\n",
    "#             p_values.append(p_value)\n",
    "\n",
    "#     # Print total significant columns\n",
    "#     print(\"\\nTotal columns with p-value <= 0.05:\", len(significant_columns))\n",
    "    \n",
    "#     # Print names of significant columns\n",
    "#     print(\"\\nColumns with p-value <= 0.05:\", significant_columns)\n",
    "    \n",
    "#     # Create a new DataFrame with only the significant columns\n",
    "#     df_significant = df[significant_columns]\n",
    "    \n",
    "#     return df_significant\n",
    "\n",
    "# # Example usage:\n",
    "# target_variable_name = 'price_doc'\n",
    "# X2 = find_significant_columns(train, target_variable_name)\n",
    "# print(\"\\nDataFrame with significant columns:\")\n",
    "# print(X2)\n",
    "\n",
    "# test2 = test[X2.columns]\n",
    "\n",
    "# X = X.astype(float)\n",
    "# X = sm.add_constant(X)\n",
    "# mod = sm.OLS(y, X)\n",
    "# res = mod.fit()\n",
    "# print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3 = X\n",
    "\n",
    "# def find_significant_columns(X, y):\n",
    "#     # Convert the entire DataFrame to float\n",
    "#     X = X.astype(float)\n",
    "\n",
    "#     significant_columns = []\n",
    "\n",
    "#     for column in X.columns:\n",
    "#         if X[column].dtype == float:  # Only consider float columns\n",
    "#             X_with_constant = sm.add_constant(X[column])\n",
    "#             model = sm.OLS(y, X_with_constant).fit()\n",
    "#             p_value = model.pvalues[1]  # Extract the p-value for the independent variable\n",
    "\n",
    "#             if p_value <= 0.05:\n",
    "#                 significant_columns.append(column)\n",
    "#                 print(f\"{column}: p-value = {p_value:.4f}\")\n",
    "\n",
    "#     print(\"\\nTotal columns with p-value <= 0.05:\", len(significant_columns))\n",
    "\n",
    "#     # Create a new DataFrame with only significant columns\n",
    "#     significant_df = X[significant_columns]\n",
    "\n",
    "#     return significant_df\n",
    "\n",
    "# X2 = find_significant_columns(X, y)\n",
    "# print(\"\\nDataFrame with significant columns:\")\n",
    "# print(X2)\n",
    "\n",
    "# # print columns in X but not in X2\n",
    "# print(\"\\nColumns in X but not in X2:\")\n",
    "# print(set(X3.columns) - set(X2.columns))\n",
    "\n",
    "# test2 = test[X2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print columns in X but not in X2\n",
    "# print(\"\\nColumns in X but not in X2:\")\n",
    "# print(set(X3.columns) - set(X2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X = X.astype(float)\n",
    "# # X = sm.add_constant(X)\n",
    "# # mod = sm.OLS(y, X)\n",
    "# # res = mod.fit()\n",
    "# # print(res.summary())\n",
    "\n",
    "# X = X.astype(float)\n",
    "# X = sm.add_constant(X)\n",
    "# mod = sm.OLS(y, X)\n",
    "# res = mod.fit()\n",
    "\n",
    "# file_name = \"output_summary.txt\"\n",
    "\n",
    "# with open(file_name, 'w') as file:\n",
    "#     print(res.summary(), file=file)\n",
    "\n",
    "# print(f\"Summary saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def find_significant_columns(file_path):\n",
    "#     # Read the file into a DataFrame\n",
    "#     df = pd.read_table(file_path, skiprows=63)  # Skip the initial rows until the column names\n",
    "\n",
    "#     # Print column names for inspection\n",
    "#     print(\"Column Names:\")\n",
    "#     print(df.columns)\n",
    "\n",
    "#     # Replace 'P>|t|' with the correct column name\n",
    "#     # Extract the column names and p-values\n",
    "#     column_names = df.columns\n",
    "#     p_values = df['Replace_With_Correct_Column_Name']\n",
    "\n",
    "#     # Find columns where p < 0.05\n",
    "#     significant_columns = column_names[p_values < 0.05]\n",
    "\n",
    "#     return significant_columns\n",
    "\n",
    "# # Replace 'output_summary.txt' with the actual file path\n",
    "# file_path = 'output_summary.txt'\n",
    "# significant_columns = find_significant_columns(file_path)\n",
    "\n",
    "# # Print or use the significant columns as needed\n",
    "# print(\"Significant Columns:\")\n",
    "# print(significant_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Poly Interaction ONNNNNNNNN<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, interaction_only=True)\n",
    "X2 = poly.fit_transform(X)\n",
    "# print(X2.shape)\n",
    "# print(poly.get_feature_names_out())\n",
    "test2 = poly.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applying Model<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 51)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 51)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.11939725e+03  1.21890673e+03  1.69238090e+05  9.16767534e-01\n",
      "   4.27180385e+00  2.62920097e+00  3.86621735e-03  2.77655862e+01\n",
      "   2.00176833e+06  5.97430710e+04 -2.59708811e+03 -5.58647528e+02\n",
      "   8.01217788e+02 -9.79871537e+02  3.01768870e+04 -1.73484780e+03\n",
      "   4.42542237e+05  1.39758868e+04  1.69622339e+05  1.97573026e+03\n",
      "  -1.16614065e+03 -7.12013533e+02 -1.44040281e+04  3.80168516e+03\n",
      "   1.32590129e+05 -2.01562422e+04  6.06374339e+03 -1.79126222e+03\n",
      "   1.94889195e+04  5.51525628e+04 -2.35677892e+04  3.26842509e+04\n",
      "   1.49780753e+04 -1.46253350e+04  2.59854637e+04 -2.49719513e+04\n",
      "   6.82260050e+04  6.45723123e+05 -3.07932656e+04  4.36197479e+03\n",
      "  -2.33081408e+04  2.19389522e+04 -2.19389522e+04 -1.19912314e+06\n",
      "   1.19912314e+06 -1.29040763e+06  1.29040763e+06 -1.55037881e+06\n",
      "   1.55037881e+06 -5.73017547e+05  5.73017547e+05]]\n",
      "[6805950.47016537]\n"
     ]
    }
   ],
   "source": [
    "reg2 = LinearRegression().fit(X2, y)\n",
    "print(reg2.coef_)\n",
    "print(reg2.intercept_)\n",
    "y_pred = reg2.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'row ID': testOriginal['row ID'], 'price_doc': y_pred.flatten()})\n",
    "result_df.to_csv('Day3.1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
