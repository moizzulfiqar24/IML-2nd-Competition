{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "testOriginal = pd.read_csv('../../2nd-Comp-Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 272)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('row ID', axis=1, inplace=True)\n",
    "test.drop('sub_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('sub_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Categorical To Numerical<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>OneHot<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Label<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     train[column] = label_encoder.fit_transform(train[column])\n",
    "\n",
    "# categorical_columns_test = test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     test[column] = label_encoder.fit_transform(test[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scaling<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_scaling_needed(dataframe, threshold=5):\n",
    "#     \"\"\"\n",
    "#     Identify columns in a pandas DataFrame that may require scaling.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Range threshold to determine if a column requires scaling (default is 5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, range_value) for columns that may require scaling.\n",
    "#     \"\"\"\n",
    "#     scaling_needed_columns = []\n",
    "\n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             column_range = dataframe[column].max() - dataframe[column].min()\n",
    "#             if column_range > threshold:\n",
    "#                 scaling_needed_columns.append((column, column_range))\n",
    "\n",
    "#     return scaling_needed_columns\n",
    "\n",
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "\n",
    "# if not scaling_needed_columns:\n",
    "#     print(\"No columns require scaling.\")\n",
    "# else:\n",
    "#     print(\"Columns that may require scaling:\")\n",
    "#     for column, column_range in scaling_needed_columns:\n",
    "#         print(f\"{column}: Range = {column_range}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_needed_columns = check_scaling_needed(train)\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# for column, _ in scaling_needed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = scaler.fit_transform(train[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_skewed_columns(dataframe, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Identify skewed columns in a pandas DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframe: pandas DataFrame\n",
    "#     - threshold: Skewness threshold to determine if a column is skewed (default is 0.5)\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing (column_name, skewness_value) for skewed columns.\n",
    "#     \"\"\"\n",
    "#     skewed_columns = []\n",
    "    \n",
    "#     for column in dataframe.columns:\n",
    "#         if dataframe[column].dtype in ['int64', 'float64']:\n",
    "#             skewness = skew(dataframe[column])\n",
    "#             if abs(skewness) > threshold:\n",
    "#                 skewed_columns.append((column, skewness))\n",
    "    \n",
    "#     return skewed_columns\n",
    "\n",
    "# skewed_columns = find_skewed_columns(train)\n",
    "\n",
    "# if not skewed_columns:\n",
    "#     print(\"No skewed columns found.\")\n",
    "# else:\n",
    "#     print(\"Skewed columns:\")\n",
    "#     for column, skewness in skewed_columns:\n",
    "#         print(f\"{column}: Skewness = {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column, _ in skewed_columns:\n",
    "#     if train[column].dtype in ['int64', 'float64']:\n",
    "#         train[column] = train[column].apply(lambda x: 1 if x == 0 else np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.loc[:, train.columns != 'price_doc']\n",
    "y = train[['price_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 287)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 287)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PCA<h3> \n",
    "<h5><i>Remember to set X2 & test2<i><h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=287)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# X2 = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "# pca2 = PCA(n_components=287)\n",
    "# principalComponents = pca2.fit_transform(test)\n",
    "# test2 = pd.DataFrame(data = principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot an elbow graph to find the optimal number of components\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(pca.explained_variance_ratio_)\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=5)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]\n",
    "# test = test[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Feature Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_reg = LinearRegression()\n",
    "# sfs = SequentialFeatureSelector(ols_reg, direction='forward',n_features_to_select=15)\n",
    "# sfs.fit(X, y)\n",
    "# print(sfs.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2 = X[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]\n",
    "# test2 = test[['full_sq', 'mosque_count_500', 'leisure_count_500', 'cafe_count_1000_price_high', 'leisure_count_1000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>P-Value Selection<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_significant_columns(df, target_variable_name):\n",
    "#     # Initialize lists to store column names and corresponding p-values\n",
    "#     significant_columns = []\n",
    "#     p_values = []\n",
    "\n",
    "#     # Loop through each column in the DataFrame\n",
    "#     for column in df.columns:\n",
    "#         # Skip the target variable itself\n",
    "#         if column == target_variable_name:\n",
    "#             continue\n",
    "        \n",
    "#         # Add a constant term for the intercept\n",
    "#         X = sm.add_constant(df[column])\n",
    "        \n",
    "#         # Fit the linear regression model\n",
    "#         model = sm.OLS(df[target_variable_name], X).fit()\n",
    "        \n",
    "#         # Get the p-value for the coefficient of the predictor variable\n",
    "#         p_value = model.pvalues[1]\n",
    "        \n",
    "#         # Print information about the current column\n",
    "#         print(f\"{column}: p-value = {p_value}\")\n",
    "        \n",
    "#         # Check if the p-value is less than or equal to 0.05\n",
    "#         if p_value <= 0.05:\n",
    "#             significant_columns.append(column)\n",
    "#             p_values.append(p_value)\n",
    "\n",
    "#     # Print total significant columns\n",
    "#     print(\"\\nTotal columns with p-value <= 0.05:\", len(significant_columns))\n",
    "    \n",
    "#     # Print names of significant columns\n",
    "#     print(\"\\nColumns with p-value <= 0.05:\", significant_columns)\n",
    "    \n",
    "#     # Create a new DataFrame with only the significant columns\n",
    "#     df_significant = df[significant_columns]\n",
    "    \n",
    "#     return df_significant\n",
    "\n",
    "# # Example usage:\n",
    "# target_variable_name = 'price_doc'\n",
    "# X2 = find_significant_columns(train, target_variable_name)\n",
    "# print(\"\\nDataFrame with significant columns:\")\n",
    "# print(X2)\n",
    "\n",
    "# test2 = test[X2.columns]\n",
    "\n",
    "# X = X.astype(float)\n",
    "# X = sm.add_constant(X)\n",
    "# mod = sm.OLS(y, X)\n",
    "# res = mod.fit()\n",
    "# print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3 = X\n",
    "\n",
    "# def find_significant_columns(X, y):\n",
    "#     # Convert the entire DataFrame to float\n",
    "#     X = X.astype(float)\n",
    "\n",
    "#     significant_columns = []\n",
    "\n",
    "#     for column in X.columns:\n",
    "#         if X[column].dtype == float:  # Only consider float columns\n",
    "#             X_with_constant = sm.add_constant(X[column])\n",
    "#             model = sm.OLS(y, X_with_constant).fit()\n",
    "#             p_value = model.pvalues[1]  # Extract the p-value for the independent variable\n",
    "\n",
    "#             if p_value <= 0.05:\n",
    "#                 significant_columns.append(column)\n",
    "#                 print(f\"{column}: p-value = {p_value:.4f}\")\n",
    "\n",
    "#     print(\"\\nTotal columns with p-value <= 0.05:\", len(significant_columns))\n",
    "\n",
    "#     # Create a new DataFrame with only significant columns\n",
    "#     significant_df = X[significant_columns]\n",
    "\n",
    "#     return significant_df\n",
    "\n",
    "# X2 = find_significant_columns(X, y)\n",
    "# print(\"\\nDataFrame with significant columns:\")\n",
    "# print(X2)\n",
    "\n",
    "# # print columns in X but not in X2\n",
    "# print(\"\\nColumns in X but not in X2:\")\n",
    "# print(set(X3.columns) - set(X2.columns))\n",
    "\n",
    "# test2 = test[X2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print columns in X but not in X2\n",
    "# print(\"\\nColumns in X but not in X2:\")\n",
    "# print(set(X3.columns) - set(X2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to output_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# X = X.astype(float)\n",
    "# X = sm.add_constant(X)\n",
    "# mod = sm.OLS(y, X)\n",
    "# res = mod.fit()\n",
    "# print(res.summary())\n",
    "\n",
    "X = X.astype(float)\n",
    "X = sm.add_constant(X)\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "\n",
    "file_name = \"output_summary.txt\"\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    print(res.summary(), file=file)\n",
    "\n",
    "print(f\"Summary saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:\n",
      "Index(['build_count_wood                        603.4439    325.078      1.856      0.063     -33.702    1240.590'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Replace_With_Correct_Column_Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Replace_With_Correct_Column_Name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th - Fall 2023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb Cell 36\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Replace 'output_summary.txt' with the actual file path\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutput_summary.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m significant_columns \u001b[39m=\u001b[39m find_significant_columns(file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Print or use the significant columns as needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSignificant Columns:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th - Fall 2023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Replace 'P>|t|' with the correct column name\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Extract the column names and p-values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m column_names \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m p_values \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mReplace_With_Correct_Column_Name\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Find columns where p < 0.05\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/moiz/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/IBA/5th%20-%20Fall%202023/IML/IML-2nd-Competition/Day2/Day2.8.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m significant_columns \u001b[39m=\u001b[39m column_names[p_values \u001b[39m<\u001b[39m \u001b[39m0.05\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Replace_With_Correct_Column_Name'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_significant_columns(file_path):\n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_table(file_path, skiprows=63)  # Skip the initial rows until the column names\n",
    "\n",
    "    # Print column names for inspection\n",
    "    print(\"Column Names:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Replace 'P>|t|' with the correct column name\n",
    "    # Extract the column names and p-values\n",
    "    column_names = df.columns\n",
    "    p_values = df['Replace_With_Correct_Column_Name']\n",
    "\n",
    "    # Find columns where p < 0.05\n",
    "    significant_columns = column_names[p_values < 0.05]\n",
    "\n",
    "    return significant_columns\n",
    "\n",
    "# Replace 'output_summary.txt' with the actual file path\n",
    "file_path = 'output_summary.txt'\n",
    "significant_columns = find_significant_columns(file_path)\n",
    "\n",
    "# Print or use the significant columns as needed\n",
    "print(\"Significant Columns:\")\n",
    "print(significant_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applying Model<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 286)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 286)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg2 = LinearRegression().fit(X_train, y_train)\n",
    "# y_pred = reg2.predict(X_test)\n",
    "# print(\"LR: R2 = %.4f and MSE = %.2f\" % (reg2.score(X_test,y_test), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = LinearRegression().fit(X2, y)\n",
    "y_pred = reg2.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'row ID': testOriginal['row ID'], 'price_doc': y_pred.flatten()})\n",
    "result_df.to_csv('Day2.8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
