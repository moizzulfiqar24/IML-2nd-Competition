9.1

Features Used = 271

Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, Hidden Layer 2 with 210 neurons relu activation L2 regularization, Hidden Layer 3 with 150 neurons relu activation L2 regularization, Hidden Layer 4 with 60 neurons relu activation L2 regularization
Output linear
Optimizer: Adam(lr=0.01)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3 (used in hidden layers only)

9.2

Features Used = 271

Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, 
Hidden Layer 2 with 210 neurons relu activation L2 regularization, 
Hidden Layer 3 with 150 neurons relu activation L2 regularization, 
Hidden Layer 4 with 60 neurons relu activation L2 regularization
Output linear
Optimizer: Adam(lr=0.01)
Loss Calculation: Mean Squared Error
batch_size=1800, epochs=100, dropout=0.3, L2 Regularization

9.3

Features Used = 271

Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, 
Hidden Layer 2 with 210 neurons relu activation L2 regularization, 
Hidden Layer 3 with 150 neurons relu activation L2 regularization, 
Hidden Layer 4 with 60 neurons relu activation L2 regularization
Output linear
Optimizer: RMSprop(lr=0.001)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

9.4

Features Used = 271

Label Encoding
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, 
Hidden Layer 2 with 210 neurons relu activation L2 regularization, 
Hidden Layer 3 with 150 neurons relu activation L2 regularization, 
Hidden Layer 4 with 60 neurons relu activation L2 regularization
Output linear
Optimizer: Adagrad(lr=0.01)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

13516578.54

9.5

