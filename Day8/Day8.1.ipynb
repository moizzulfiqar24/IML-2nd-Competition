{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test_data = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "\n",
    "used = []\n",
    "\n",
    "# Extract features and target variable\n",
    "X = train_data.drop('price_doc', axis=1)\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.drop('sub_area', axis=1)\n",
    "# X_test = X_test.drop('sub_area', axis=1)\n",
    "# used.append('Removed sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.get_dummies(X)\n",
    "# X_test = pd.get_dummies(X_test) \n",
    "# used.append('OneHot Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Categorical columns: ['product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion', 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', 'water_1line', 'big_road1_1line', 'railroad_1line', 'ecology']\n",
      "Test: Categorical columns: ['product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion', 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', 'water_1line', 'big_road1_1line', 'railroad_1line', 'ecology']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "categorical_columns_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns_test:\n",
    "    X_test[column] = label_encoder.fit_transform(X_test[column])\n",
    "\n",
    "used.append('Label Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all columns in X_train with dtypes object\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         X.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# # drop all columns in X_test with dtypes object\n",
    "# for col in X_test.columns:\n",
    "#     if X_test[col].dtype == 'object':\n",
    "#         X_test.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# used.append(\"Removed Object Dtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.log1p(X)\n",
    "X_test = np.log1p(X_test)\n",
    "used.append('log Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "used.append(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>With Keras<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_used = X_train.shape[1]\n",
    "\n",
    "# # optimizerUsing = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# # optimizerUsing = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "# optimizerUsing = Adam(lr=0.001)#, epsilon=1e-07)\n",
    "\n",
    "# # Build the neural network\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))  # You can use either dropout or early stopping\n",
    "# model.add(Dense(68, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(40, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1, activation='linear'))  # Output layer with linear activation for regression\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizerUsing)\n",
    "\n",
    "# # Define early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=1800, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# used.append(\"Keras With Torch Backend\")\n",
    "# # used.append(\"1 Hidden Layer: Hidden Layer 1 with 140 neurons relu activation, Output linear\")\n",
    "# # used.append(\"2 Hidden Layers: Hidden Layer 1 with 100 neurons relu activation, Hidden Layer 2 with 50 neurons relu activation, Dropout 0.3, Output linear\")\n",
    "# used.append(\"3 Hidden Layers: Hidden Layer 1 with 128 neurons relu activation L2 regularization, Hidden Layer 2 with 64 neurons relu activation L2 regularization, Hidden Layer 3 with 40 neurons relu activation L2 regularization\")\n",
    "# used.append(\"Output linear\")\n",
    "# used.append(\"Optimizer: Adam(lr=0.01)\")\n",
    "# used.append(\"Loss Calculation: Mean Squared Error\")\n",
    "# used.append(\"EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\")\n",
    "# used.append(\"batch_size=1800, epochs=100, early_stopping, dropout=0.3 (used in hidden layers only)\")\n",
    "# # used.append(\"batch_size=1800, epochs=100, early_stopping\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>With Skilearn<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 347809336745262.18750000\n",
      "Validation score: -0.452463\n",
      "Iteration 2, loss = 347571358890423.43750000\n",
      "Validation score: -0.449599\n",
      "Iteration 3, loss = 345612762750481.43750000\n",
      "Validation score: -0.433104\n",
      "Iteration 4, loss = 338105629505757.37500000\n",
      "Validation score: -0.381921\n",
      "Iteration 5, loss = 319151587610581.00000000\n",
      "Validation score: -0.269652\n",
      "Iteration 6, loss = 283159437970366.87500000\n",
      "Validation score: -0.080040\n",
      "Iteration 7, loss = 229908756366438.37500000\n",
      "Validation score: 0.167306\n",
      "Iteration 8, loss = 169986187922563.43750000\n",
      "Validation score: 0.402627\n",
      "Iteration 9, loss = 122511494927217.17187500\n",
      "Validation score: 0.549031\n",
      "Iteration 10, loss = 99239372625398.06250000\n",
      "Validation score: 0.598737\n",
      "Iteration 11, loss = 92925296001839.73437500\n",
      "Validation score: 0.608128\n",
      "Iteration 12, loss = 91755692837676.34375000\n",
      "Validation score: 0.610364\n",
      "Iteration 13, loss = 91322222547419.15625000\n",
      "Validation score: 0.611913\n",
      "Iteration 14, loss = 90975598177520.43750000\n",
      "Validation score: 0.613275\n",
      "Iteration 15, loss = 90670714526610.12500000\n",
      "Validation score: 0.614518\n",
      "Iteration 16, loss = 90398110744582.51562500\n",
      "Validation score: 0.615667\n",
      "Iteration 17, loss = 90151985180205.42187500\n",
      "Validation score: 0.616689\n",
      "Iteration 18, loss = 89920687065694.89062500\n",
      "Validation score: 0.617621\n",
      "Iteration 19, loss = 89713906354716.00000000\n",
      "Validation score: 0.618484\n",
      "Iteration 20, loss = 89518498983916.98437500\n",
      "Validation score: 0.619279\n",
      "Iteration 21, loss = 89340239706893.53125000\n",
      "Validation score: 0.620025\n",
      "Iteration 22, loss = 89175997975452.64062500\n",
      "Validation score: 0.620678\n",
      "Iteration 23, loss = 89025336944508.53125000\n",
      "Validation score: 0.621318\n",
      "Iteration 24, loss = 88881978638759.96875000\n",
      "Validation score: 0.621904\n",
      "Iteration 25, loss = 88748690197212.75000000\n",
      "Validation score: 0.622444\n",
      "Iteration 26, loss = 88626642570537.12500000\n",
      "Validation score: 0.622940\n",
      "Iteration 27, loss = 88516100372706.57812500\n",
      "Validation score: 0.623400\n",
      "Iteration 28, loss = 88403028618309.78125000\n",
      "Validation score: 0.623823\n",
      "Iteration 29, loss = 88303489267729.51562500\n",
      "Validation score: 0.624239\n",
      "Iteration 30, loss = 88204774644051.65625000\n",
      "Validation score: 0.624617\n",
      "Iteration 31, loss = 88121092015909.45312500\n",
      "Validation score: 0.624991\n",
      "Iteration 32, loss = 88028383453207.60937500\n",
      "Validation score: 0.625327\n",
      "Iteration 33, loss = 87949436390704.96875000\n",
      "Validation score: 0.625581\n",
      "Iteration 34, loss = 87871082981784.35937500\n",
      "Validation score: 0.625959\n",
      "Iteration 35, loss = 87793324443538.68750000\n",
      "Validation score: 0.626226\n",
      "Iteration 36, loss = 87720793977342.60937500\n",
      "Validation score: 0.626524\n",
      "Iteration 37, loss = 87654845714942.67187500\n",
      "Validation score: 0.626775\n",
      "Iteration 38, loss = 87582716430360.75000000\n",
      "Validation score: 0.627073\n",
      "Iteration 39, loss = 87519391025411.10937500\n",
      "Validation score: 0.627299\n",
      "Iteration 40, loss = 87459106898288.51562500\n",
      "Validation score: 0.627449\n",
      "Iteration 41, loss = 87396370669185.29687500\n",
      "Validation score: 0.627809\n",
      "Iteration 42, loss = 87335446758902.68750000\n",
      "Validation score: 0.628000\n",
      "Iteration 43, loss = 87278123268999.82812500\n",
      "Validation score: 0.628237\n",
      "Iteration 44, loss = 87225413732965.15625000\n",
      "Validation score: 0.628436\n",
      "Iteration 45, loss = 87180745820802.85937500\n",
      "Validation score: 0.628615\n",
      "Iteration 46, loss = 87113795323283.32812500\n",
      "Validation score: 0.628842\n",
      "Iteration 47, loss = 87072727874268.67187500\n",
      "Validation score: 0.629008\n",
      "Iteration 48, loss = 87019608578406.06250000\n",
      "Validation score: 0.629207\n",
      "Iteration 49, loss = 86969256491450.67187500\n",
      "Validation score: 0.629266\n",
      "Iteration 50, loss = 86938276349833.71875000\n",
      "Validation score: 0.629448\n",
      "Iteration 51, loss = 86879852202643.26562500\n",
      "Validation score: 0.629687\n",
      "Iteration 52, loss = 86834572491432.29687500\n",
      "Validation score: 0.629841\n",
      "Iteration 53, loss = 86797971266146.09375000\n",
      "Validation score: 0.630024\n",
      "Iteration 54, loss = 86745822056099.65625000\n",
      "Validation score: 0.630144\n",
      "Iteration 55, loss = 86714943072318.90625000\n",
      "Validation score: 0.630324\n",
      "Iteration 56, loss = 86674807218044.20312500\n",
      "Validation score: 0.630478\n",
      "Iteration 57, loss = 86629995655929.04687500\n",
      "Validation score: 0.630558\n",
      "Iteration 58, loss = 86589064751299.81250000\n",
      "Validation score: 0.630751\n",
      "Iteration 59, loss = 86537886271391.51562500\n",
      "Validation score: 0.630887\n",
      "Iteration 60, loss = 86503984906449.68750000\n",
      "Validation score: 0.631029\n",
      "Iteration 61, loss = 86469061160741.42187500\n",
      "Validation score: 0.631143\n",
      "Iteration 62, loss = 86421643687177.81250000\n",
      "Validation score: 0.631305\n",
      "Iteration 63, loss = 86379868573861.17187500\n",
      "Validation score: 0.631375\n",
      "Iteration 64, loss = 86363675694079.65625000\n",
      "Validation score: 0.631511\n",
      "Iteration 65, loss = 86311569032876.42187500\n",
      "Validation score: 0.631665\n",
      "Iteration 66, loss = 86274513003116.39062500\n",
      "Validation score: 0.631779\n",
      "Iteration 67, loss = 86239313210612.21875000\n",
      "Validation score: 0.631806\n",
      "Iteration 68, loss = 86197043211518.23437500\n",
      "Validation score: 0.632038\n",
      "Iteration 69, loss = 86169628218334.56250000\n",
      "Validation score: 0.632167\n",
      "Iteration 70, loss = 86132958241692.93750000\n",
      "Validation score: 0.632277\n",
      "Iteration 71, loss = 86096308852256.48437500\n",
      "Validation score: 0.632407\n",
      "Iteration 72, loss = 86059235792372.09375000\n",
      "Validation score: 0.632403\n",
      "Iteration 73, loss = 86026737394816.48437500\n",
      "Validation score: 0.632611\n",
      "Iteration 74, loss = 85986295027328.85937500\n",
      "Validation score: 0.632714\n",
      "Iteration 75, loss = 85960224987114.85937500\n",
      "Validation score: 0.632848\n",
      "Iteration 76, loss = 85918651582673.54687500\n",
      "Validation score: 0.632929\n",
      "Iteration 77, loss = 85905447247862.59375000\n",
      "Validation score: 0.632915\n",
      "Iteration 78, loss = 85858527397358.09375000\n",
      "Validation score: 0.633160\n",
      "Iteration 79, loss = 85821203554411.60937500\n",
      "Validation score: 0.633270\n",
      "Iteration 80, loss = 85783507911306.84375000\n",
      "Validation score: 0.633293\n",
      "Iteration 81, loss = 85748561688859.62500000\n",
      "Validation score: 0.633461\n",
      "Iteration 82, loss = 85719453175356.82812500\n",
      "Validation score: 0.633373\n",
      "Iteration 83, loss = 85685618778579.31250000\n",
      "Validation score: 0.633712\n",
      "Iteration 84, loss = 85660076283509.82812500\n",
      "Validation score: 0.633816\n",
      "Iteration 85, loss = 85622714739807.53125000\n",
      "Validation score: 0.633877\n",
      "Iteration 86, loss = 85580474840075.56250000\n",
      "Validation score: 0.634021\n",
      "Iteration 87, loss = 85555942699575.18750000\n",
      "Validation score: 0.634107\n",
      "Iteration 88, loss = 85524771156924.98437500\n",
      "Validation score: 0.634216\n",
      "Iteration 89, loss = 85495551910483.59375000\n",
      "Validation score: 0.634283\n",
      "Iteration 90, loss = 85459533690651.81250000\n",
      "Validation score: 0.634364\n",
      "Iteration 91, loss = 85439873167677.64062500\n",
      "Validation score: 0.634432\n",
      "Iteration 92, loss = 85390871403282.75000000\n",
      "Validation score: 0.634541\n",
      "Iteration 93, loss = 85367728360530.40625000\n",
      "Validation score: 0.634696\n",
      "Iteration 94, loss = 85337602455898.35937500\n",
      "Validation score: 0.634796\n",
      "Iteration 95, loss = 85297455739596.57812500\n",
      "Validation score: 0.634842\n",
      "Iteration 96, loss = 85287472135578.39062500\n",
      "Validation score: 0.634730\n",
      "Iteration 97, loss = 85243783083080.78125000\n",
      "Validation score: 0.634983\n",
      "Iteration 98, loss = 85213517638614.23437500\n",
      "Validation score: 0.635111\n",
      "Iteration 99, loss = 85179468295498.60937500\n",
      "Validation score: 0.635180\n",
      "Iteration 100, loss = 85149225892472.10937500\n",
      "Validation score: 0.635333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features_used = X_train.shape[1]\n",
    "\n",
    "# # optimizerUsing = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# # optimizerUsing = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "# optimizerUsing = Adam(lr=0.001)#, epsilon=1e-07)\n",
    "\n",
    "# Build the neural network using MLPRegressor from scikit-learn\n",
    "model = MLPRegressor(hidden_layer_sizes=(128,64,40), activation='relu', solver='adam', shuffle=True, max_iter=100,\n",
    "                     batch_size=1800, early_stopping=True, verbose=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "used.append(\"MLPRegressor(hidden_layer_sizes=(128,64,40), activation='relu', solver='adam', shuffle=True, max_iter=100, batch_size=1800, early_stopping=True, verbose=True, random_state=42)\")\n",
    "# used.append(\"1 Hidden Layer: Hidden Layer 1 with 140 neurons relu activation, Output linear\")\n",
    "# used.append(\"2 Hidden Layers: Hidden Layer 1 with 100 neurons relu activation, Hidden Layer 2 with 50 neurons relu activation, Dropout 0.3, Output linear\")\n",
    "used.append(\"3 Hidden Layers: Hidden Layer 1 with 128 neurons, Hidden Layer 2 with 64 neurons, Hidden Layer 3 with 40 neurons\")\n",
    "# used.append(\"Output linear\")\n",
    "used.append(\"Optimizer: Adam\")\n",
    "# used.append(\"Loss Calculation: Mean Squared Error\")\n",
    "# used.append(\"EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\")\n",
    "# used.append(\"batch_size=1800, epochs=100, early_stopping, dropout=0.3 (used in hidden layers only)\")\n",
    "used.append(\"batch_size=1800, epochs=100, early_stopping\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({'row ID': test_data['row ID'], 'price_doc': predictions.flatten()})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "submission_df.to_csv('Day8.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used = 271\n",
      "\n",
      "Label Encoding\n",
      "Converted All Columns To float32\n",
      "log Normalization\n",
      "StandardScaler\n",
      "MLPRegressor(hidden_layer_sizes=(128,64,40), activation='relu', solver='adam', shuffle=True, max_iter=100, batch_size=1800, early_stopping=True, verbose=True, random_state=42)\n",
      "3 Hidden Layers: Hidden Layer 1 with 128 neurons, Hidden Layer 2 with 64 neurons, Hidden Layer 3 with 40 neurons\n",
      "Optimizer: Adam\n",
      "batch_size=1800, epochs=100, early_stopping\n"
     ]
    }
   ],
   "source": [
    "print(\"Features Used = \" + str(features_used) + \"\\n\")\n",
    "for i in used:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
