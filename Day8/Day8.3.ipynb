{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../../2nd-Comp-Data/train.csv')\n",
    "test_data = pd.read_csv('../../2nd-Comp-Data/test.csv')\n",
    "\n",
    "used = []\n",
    "\n",
    "# Extract features and target variable\n",
    "X = train_data.drop('price_doc', axis=1)\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('sub_area', axis=1)\n",
    "X_test = X_test.drop('sub_area', axis=1)\n",
    "used.append('Removed sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(X_test) \n",
    "used.append('OneHot Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import LabelEncoder\n",
    "\n",
    "# categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Train: Categorical columns:\", categorical_columns)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# categorical_columns_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(\"Test: Categorical columns:\", categorical_columns_test)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for column in categorical_columns_test:\n",
    "#     X_test[column] = label_encoder.fit_transform(X_test[column])\n",
    "\n",
    "# used.append('Label Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all columns in X_train with dtypes object\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         X.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# # drop all columns in X_test with dtypes object\n",
    "# for col in X_test.columns:\n",
    "#     if X_test[col].dtype == 'object':\n",
    "#         X_test.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# used.append(\"Removed Object Dtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "used.append(\"Converted All Columns To float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.log1p(X)\n",
    "X_test = np.log1p(X_test)\n",
    "used.append('log Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "used.append(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>With Keras<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_used = X_train.shape[1]\n",
    "\n",
    "# # optimizerUsing = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# # optimizerUsing = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "# optimizerUsing = Adam(lr=0.001)#, epsilon=1e-07)\n",
    "\n",
    "# # Build the neural network\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))  # You can use either dropout or early stopping\n",
    "# model.add(Dense(68, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(40, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1, activation='linear'))  # Output layer with linear activation for regression\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizerUsing)\n",
    "\n",
    "# # Define early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=1800, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# used.append(\"Keras With Torch Backend\")\n",
    "# # used.append(\"1 Hidden Layer: Hidden Layer 1 with 140 neurons relu activation, Output linear\")\n",
    "# # used.append(\"2 Hidden Layers: Hidden Layer 1 with 100 neurons relu activation, Hidden Layer 2 with 50 neurons relu activation, Dropout 0.3, Output linear\")\n",
    "# used.append(\"3 Hidden Layers: Hidden Layer 1 with 128 neurons relu activation L2 regularization, Hidden Layer 2 with 64 neurons relu activation L2 regularization, Hidden Layer 3 with 40 neurons relu activation L2 regularization\")\n",
    "# used.append(\"Output linear\")\n",
    "# used.append(\"Optimizer: Adam(lr=0.01)\")\n",
    "# used.append(\"Loss Calculation: Mean Squared Error\")\n",
    "# used.append(\"EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\")\n",
    "# used.append(\"batch_size=1800, epochs=100, early_stopping, dropout=0.3 (used in hidden layers only)\")\n",
    "# # used.append(\"batch_size=1800, epochs=100, early_stopping\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>With Skilearn<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 347657753013316.50000000\n",
      "Validation score: -0.457934\n",
      "Iteration 2, loss = 347312653926224.50000000\n",
      "Validation score: -0.453643\n",
      "Iteration 3, loss = 344330781543349.75000000\n",
      "Validation score: -0.428189\n",
      "Iteration 4, loss = 332756486314499.18750000\n",
      "Validation score: -0.349074\n",
      "Iteration 5, loss = 304054571187005.43750000\n",
      "Validation score: -0.180384\n",
      "Iteration 6, loss = 252316747387953.62500000\n",
      "Validation score: 0.082547\n",
      "Iteration 7, loss = 184908477469641.84375000\n",
      "Validation score: 0.367556\n",
      "Iteration 8, loss = 126788763396157.43750000\n",
      "Validation score: 0.552685\n",
      "Iteration 9, loss = 99026118496928.32812500\n",
      "Validation score: 0.609511\n",
      "Iteration 10, loss = 92890919482458.04687500\n",
      "Validation score: 0.617774\n",
      "Iteration 11, loss = 92021465064450.17187500\n",
      "Validation score: 0.619821\n",
      "Iteration 12, loss = 91656999642717.65625000\n",
      "Validation score: 0.621319\n",
      "Iteration 13, loss = 91349305105331.32812500\n",
      "Validation score: 0.622654\n",
      "Iteration 14, loss = 91067903619843.62500000\n",
      "Validation score: 0.623891\n",
      "Iteration 15, loss = 90807383217891.85937500\n",
      "Validation score: 0.625026\n",
      "Iteration 16, loss = 90568204072740.71875000\n",
      "Validation score: 0.626064\n",
      "Iteration 17, loss = 90349276524902.17187500\n",
      "Validation score: 0.627021\n",
      "Iteration 18, loss = 90145765009954.31250000\n",
      "Validation score: 0.627921\n",
      "Iteration 19, loss = 89954638676083.87500000\n",
      "Validation score: 0.628767\n",
      "Iteration 20, loss = 89775039269373.92187500\n",
      "Validation score: 0.629564\n",
      "Iteration 21, loss = 89606935854040.40625000\n",
      "Validation score: 0.630309\n",
      "Iteration 22, loss = 89450024424902.07812500\n",
      "Validation score: 0.631008\n",
      "Iteration 23, loss = 89302662446293.48437500\n",
      "Validation score: 0.631665\n",
      "Iteration 24, loss = 89163995908797.10937500\n",
      "Validation score: 0.632283\n",
      "Iteration 25, loss = 89033373480091.71875000\n",
      "Validation score: 0.632864\n",
      "Iteration 26, loss = 88911241408671.90625000\n",
      "Validation score: 0.633407\n",
      "Iteration 27, loss = 88796670952459.26562500\n",
      "Validation score: 0.633918\n",
      "Iteration 28, loss = 88688643673956.79687500\n",
      "Validation score: 0.634398\n",
      "Iteration 29, loss = 88586544035275.23437500\n",
      "Validation score: 0.634851\n",
      "Iteration 30, loss = 88489455398644.15625000\n",
      "Validation score: 0.635280\n",
      "Iteration 31, loss = 88397296610313.95312500\n",
      "Validation score: 0.635687\n",
      "Iteration 32, loss = 88309999703437.51562500\n",
      "Validation score: 0.636071\n",
      "Iteration 33, loss = 88226828078561.10937500\n",
      "Validation score: 0.636435\n",
      "Iteration 34, loss = 88147329802876.85937500\n",
      "Validation score: 0.636782\n",
      "Iteration 35, loss = 88071155674766.59375000\n",
      "Validation score: 0.637112\n",
      "Iteration 36, loss = 87998046802300.56250000\n",
      "Validation score: 0.637429\n",
      "Iteration 37, loss = 87927732017922.03125000\n",
      "Validation score: 0.637731\n",
      "Iteration 38, loss = 87860157011992.78125000\n",
      "Validation score: 0.638021\n",
      "Iteration 39, loss = 87795308946621.56250000\n",
      "Validation score: 0.638298\n",
      "Iteration 40, loss = 87732814382096.35937500\n",
      "Validation score: 0.638563\n",
      "Iteration 41, loss = 87672505811098.28125000\n",
      "Validation score: 0.638818\n",
      "Iteration 42, loss = 87614234179922.34375000\n",
      "Validation score: 0.639063\n",
      "Iteration 43, loss = 87557856399131.81250000\n",
      "Validation score: 0.639298\n",
      "Iteration 44, loss = 87503233744646.29687500\n",
      "Validation score: 0.639525\n",
      "Iteration 45, loss = 87450231656384.76562500\n",
      "Validation score: 0.639745\n",
      "Iteration 46, loss = 87398722284741.90625000\n",
      "Validation score: 0.639956\n",
      "Iteration 47, loss = 87348605329784.78125000\n",
      "Validation score: 0.640161\n",
      "Iteration 48, loss = 87299771879217.93750000\n",
      "Validation score: 0.640359\n",
      "Iteration 49, loss = 87252097690211.95312500\n",
      "Validation score: 0.640552\n",
      "Iteration 50, loss = 87205506669452.48437500\n",
      "Validation score: 0.640739\n",
      "Iteration 51, loss = 87159922606224.75000000\n",
      "Validation score: 0.640921\n",
      "Iteration 52, loss = 87115273402921.18750000\n",
      "Validation score: 0.641098\n",
      "Iteration 53, loss = 87071492451727.67187500\n",
      "Validation score: 0.641271\n",
      "Iteration 54, loss = 87028506473978.14062500\n",
      "Validation score: 0.641440\n",
      "Iteration 55, loss = 86986285822391.64062500\n",
      "Validation score: 0.641604\n",
      "Iteration 56, loss = 86944811169216.32812500\n",
      "Validation score: 0.641765\n",
      "Iteration 57, loss = 86904049312642.31250000\n",
      "Validation score: 0.641922\n",
      "Iteration 58, loss = 86863921081979.71875000\n",
      "Validation score: 0.642076\n",
      "Iteration 59, loss = 86824401837635.45312500\n",
      "Validation score: 0.642227\n",
      "Iteration 60, loss = 86785478902794.40625000\n",
      "Validation score: 0.642375\n",
      "Iteration 61, loss = 86747119351085.60937500\n",
      "Validation score: 0.642520\n",
      "Iteration 62, loss = 86709309849896.48437500\n",
      "Validation score: 0.642662\n",
      "Iteration 63, loss = 86672012890777.15625000\n",
      "Validation score: 0.642802\n",
      "Iteration 64, loss = 86635182002338.79687500\n",
      "Validation score: 0.642940\n",
      "Iteration 65, loss = 86598816207360.45312500\n",
      "Validation score: 0.643074\n",
      "Iteration 66, loss = 86562931504718.09375000\n",
      "Validation score: 0.643207\n",
      "Iteration 67, loss = 86527480196963.17187500\n",
      "Validation score: 0.643337\n",
      "Iteration 68, loss = 86492412090063.31250000\n",
      "Validation score: 0.643465\n",
      "Iteration 69, loss = 86457703608493.03125000\n",
      "Validation score: 0.643591\n",
      "Iteration 70, loss = 86423326517852.54687500\n",
      "Validation score: 0.643716\n",
      "Iteration 71, loss = 86389270096096.90625000\n",
      "Validation score: 0.643838\n",
      "Iteration 72, loss = 86355514248461.87500000\n",
      "Validation score: 0.643959\n",
      "Iteration 73, loss = 86322048792849.03125000\n",
      "Validation score: 0.644079\n",
      "Iteration 74, loss = 86288831963277.35937500\n",
      "Validation score: 0.644197\n",
      "Iteration 75, loss = 86255872275894.98437500\n",
      "Validation score: 0.644314\n",
      "Iteration 76, loss = 86223155371965.35937500\n",
      "Validation score: 0.644429\n",
      "Iteration 77, loss = 86190647343333.68750000\n",
      "Validation score: 0.644543\n",
      "Iteration 78, loss = 86158352437709.87500000\n",
      "Validation score: 0.644656\n",
      "Iteration 79, loss = 86126267914418.43750000\n",
      "Validation score: 0.644769\n",
      "Iteration 80, loss = 86094376867936.76562500\n",
      "Validation score: 0.644880\n",
      "Iteration 81, loss = 86062656701331.29687500\n",
      "Validation score: 0.644991\n",
      "Iteration 82, loss = 86031081040790.17187500\n",
      "Validation score: 0.645101\n",
      "Iteration 83, loss = 85999656085812.87500000\n",
      "Validation score: 0.645210\n",
      "Iteration 84, loss = 85968367392253.93750000\n",
      "Validation score: 0.645318\n",
      "Iteration 85, loss = 85937268938023.48437500\n",
      "Validation score: 0.645426\n",
      "Iteration 86, loss = 85906366967940.53125000\n",
      "Validation score: 0.645533\n",
      "Iteration 87, loss = 85875573452943.81250000\n",
      "Validation score: 0.645639\n",
      "Iteration 88, loss = 85844931276687.60937500\n",
      "Validation score: 0.645744\n",
      "Iteration 89, loss = 85814422287394.42187500\n",
      "Validation score: 0.645848\n",
      "Iteration 90, loss = 85784140953739.79687500\n",
      "Validation score: 0.645951\n",
      "Iteration 91, loss = 85753870570259.70312500\n",
      "Validation score: 0.646054\n",
      "Iteration 92, loss = 85723695660852.45312500\n",
      "Validation score: 0.646155\n",
      "Iteration 93, loss = 85693621658674.25000000\n",
      "Validation score: 0.646256\n",
      "Iteration 94, loss = 85663703895063.42187500\n",
      "Validation score: 0.646355\n",
      "Iteration 95, loss = 85633985251331.56250000\n",
      "Validation score: 0.646452\n",
      "Iteration 96, loss = 85604512731912.15625000\n",
      "Validation score: 0.646547\n",
      "Iteration 97, loss = 85575165252077.28125000\n",
      "Validation score: 0.646640\n",
      "Iteration 98, loss = 85545908116822.14062500\n",
      "Validation score: 0.646732\n",
      "Iteration 99, loss = 85516851369832.42187500\n",
      "Validation score: 0.646825\n",
      "Iteration 100, loss = 85488012158474.43750000\n",
      "Validation score: 0.646916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features_used = X_train.shape[1]\n",
    "\n",
    "# # optimizerUsing = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# # optimizerUsing = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "# optimizerUsing = Adam(lr=0.001)#, epsilon=1e-07)\n",
    "\n",
    "# Build the neural network using MLPRegressor from scikit-learn\n",
    "model = MLPRegressor(hidden_layer_sizes=(128,64,40), activation='relu', solver='adam', shuffle=False, max_iter=100,\n",
    "                     batch_size=1800, early_stopping=True, verbose=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "used.append(\"MLPRegressor(hidden_layer_sizes=(128,64,40), activation='relu', solver='adam', shuffle=False, max_iter=100, batch_size=1800, early_stopping=True, verbose=True, random_state=42)\")\n",
    "# used.append(\"1 Hidden Layer: Hidden Layer 1 with 140 neurons relu activation, Output linear\")\n",
    "# used.append(\"2 Hidden Layers: Hidden Layer 1 with 100 neurons relu activation, Hidden Layer 2 with 50 neurons relu activation, Dropout 0.3, Output linear\")\n",
    "used.append(\"3 Hidden Layers: Hidden Layer 1 with 128 neurons, Hidden Layer 2 with 64 neurons, Hidden Layer 3 with 40 neurons\")\n",
    "# used.append(\"Output linear\")\n",
    "used.append(\"Optimizer: Adam\")\n",
    "# used.append(\"Loss Calculation: Mean Squared Error\")\n",
    "# used.append(\"EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\")\n",
    "# used.append(\"batch_size=1800, epochs=100, early_stopping, dropout=0.3 (used in hidden layers only)\")\n",
    "used.append(\"batch_size=1800, epochs=100, early_stopping\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({'row ID': test_data['row ID'], 'price_doc': predictions.flatten()})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "submission_df.to_csv('Day8.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used = 287\n",
      "\n",
      "Removed sub_area\n",
      "OneHot Encoding\n",
      "Converted All Columns To float32\n",
      "log Normalization\n",
      "StandardScaler\n",
      "MLPRegressor(hidden_layer_sizes=(128,64,40), activation='relu', solver='adam', shuffle=False, max_iter=100, batch_size=1800, early_stopping=True, verbose=True, random_state=42)\n",
      "3 Hidden Layers: Hidden Layer 1 with 128 neurons, Hidden Layer 2 with 64 neurons, Hidden Layer 3 with 40 neurons\n",
      "Optimizer: Adam\n",
      "batch_size=1800, epochs=100, early_stopping\n"
     ]
    }
   ],
   "source": [
    "print(\"Features Used = \" + str(features_used) + \"\\n\")\n",
    "for i in used:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
