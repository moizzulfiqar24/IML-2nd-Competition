12.1

Features Used = 14

Forward Feature Selection (n=14)
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.4)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

12.2

Features Used = 41

Removed sub_area
Label Encoding
Variance Based Feature Selection
Converted All Columns To float32
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.4)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

12.3

Features Used = 77

Removed sub_area
Label Encoding
Converted All Columns To float32
P-Value Based Feature Selection
log Normalization
StandardScaler
Keras With Torch Backend
4 Hidden Layers: Hidden Layer 1 with 360 neurons selu activation L2 regularization, 
Hidden Layer 2 with 280 neurons selu activation L2 regularization, 
Hidden Layer 3 with 190 neurons selu activation L2 regularization, 
Hidden Layer 4 with 130 neurons selu activation L2 regularization
Output linear
Optimizer: Ftrl(learning_rate=0.2)
Loss Calculation: Mean Squared Error
EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size=1800, epochs=100, early_stopping (patience 10), dropout=0.3, L2 Regularization

12.4

Features Used = 14

Forward Feature Selection (n=14)
Converted All Columns To float32
log Normalization
StandardScaler
MLPRegressor
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, 
Hidden Layer 2 with 210 neurons relu activation L2 regularization, 
Hidden Layer 3 with 150 neurons relu activation L2 regularization, 
Hidden Layer 4 with 60 neurons relu activation L2 regularization
Optimizer: adam
batch_size=1800, epochs=100, early_stopping

12.5

Features Used = 41

Removed sub_area
Label Encoding
Variance Based Feature Selection
Converted All Columns To float32
log Normalization
StandardScaler
MLPRegressor
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, 
Hidden Layer 2 with 210 neurons relu activation L2 regularization, 
Hidden Layer 3 with 150 neurons relu activation L2 regularization, 
Hidden Layer 4 with 60 neurons relu activation L2 regularization
Optimizer: adam
batch_size=1800, epochs=100, early_stopping

12.6

Features Used = 77

Removed sub_area
Label Encoding
P-Value Based Feature Selection
Converted All Columns To float32
log Normalization
StandardScaler
MLPRegressor
4 Hidden Layers: Hidden Layer 1 with 300 neurons relu activation L2 regularization, 
Hidden Layer 2 with 210 neurons relu activation L2 regularization, 
Hidden Layer 3 with 150 neurons relu activation L2 regularization, 
Hidden Layer 4 with 60 neurons relu activation L2 regularization
Optimizer: adam
batch_size=1800, epochs=100, early_stopping